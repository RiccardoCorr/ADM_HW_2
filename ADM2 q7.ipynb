{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a8a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6908b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_authors = \"lighter_authors.json\"\n",
    "ch_size = 100\n",
    "dfs = []\n",
    "\n",
    "# to import only some columns\n",
    "with open(light_authors, \"r\") as file:\n",
    "    for ch in pd.read_json(file, lines = True, chunksize = ch_size):\n",
    "        ch = ch[[\"name\", \"works_count\",\"ratings_count\",\"average_rating\",\n",
    "\"text_reviews_count\", \"work_ids\", \"book_ids\",\"id\",\n",
    "\"gender\",\n",
    "\"about\",\"fans_count\"]]\n",
    "        dfs.append(ch)\n",
    "\n",
    "la = pd.concat(dfs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12f6c4",
   "metadata": {},
   "source": [
    "## [RQ7]  Estimating probabilities is a core skill for a data scientist: show us your best!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761d9cf",
   "metadata": {},
   "source": [
    "* Estimate the probability that a book has over 30% of the ratings above 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90bf7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "lighter_books = \"lighter_books.json\"\n",
    "ch_size = 100\n",
    "dfbs = []\n",
    "\n",
    "# we import the dataset\n",
    "with open(lighter_books, \"r\") as fileb:\n",
    "    rows_read = 0\n",
    "    for chb in pd.read_json(fileb, lines = True, chunksize = ch_size):\n",
    "        chb = chb[[\"id\", \"title\", \"author_name\",\"author_id\", \"work_id\",\n",
    "\"language\",\"average_rating\",\"rating_dist\",\"ratings_count\",\"text_reviews_count\",\"publication_date\",\n",
    "\"original_publication_date\", \"format\",\"edition_information\",\"publisher\",\"num_pages\",\n",
    "\"series_id\",\"series_name\",\"series_position\"]]\n",
    "        dfbs.append(chb)\n",
    "#       Update the count of rows read\n",
    "        rows_read += len(chb)\n",
    "        \n",
    "        # Check if we have read 10,000 rows, and if so, break the loop\n",
    "        if rows_read >= 100000:\n",
    "            break\n",
    "            \n",
    "#final data with our books\n",
    "lb = pd.concat(dfbs, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9522b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12344\\3938496938.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dr['rating_dist'] = dr['rating_dist'].astype(str)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12344\\3938496938.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dr['ratings_4'], dr['ratings_5'] = zip(*dr['rating_dist'].map(parse_rating_dist))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12344\\3938496938.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dr['ratings_4'], dr['ratings_5'] = zip(*dr['rating_dist'].map(parse_rating_dist))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12344\\3938496938.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dr['proportion_above_4'] = (dr['ratings_4'] + dr['ratings_5']) / dr['ratings_count']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9565564925462612\n",
      "Estimated probability that a book has over 30% of ratings above 4: 95.66%\n",
      "The probability to have over 30% of above 4 is 95.65564925462611 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#we chose the data we want to study, in this case lighter_books\n",
    "dr = lb[['rating_dist','ratings_count']]\n",
    "\n",
    "#we change rating_dist to str to be able to separate the values of each rating\n",
    "dr['rating_dist'] = dr['rating_dist'].astype(str)\n",
    "\n",
    "\n",
    "#we define a function to separate the ratings we want\n",
    "def parse_rating_dist(rating_dist):\n",
    "    match = re.search(r'5:(\\d+)', rating_dist)\n",
    "    if match:\n",
    "        ratings_5 = int(match.group(1))\n",
    "    else:\n",
    "        ratings_5 = 0\n",
    "\n",
    "    match = re.search(r'4:(\\d+)', rating_dist)\n",
    "    if match:\n",
    "        ratings_4 = int(match.group(1))\n",
    "    else:\n",
    "        ratings_4 = 0  \n",
    "\n",
    "    return ratings_4, ratings_5\n",
    "\n",
    "# Apply the parsing function\n",
    "dr['ratings_4'], dr['ratings_5'] = zip(*dr['rating_dist'].map(parse_rating_dist))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the probability\n",
    "dr['proportion_above_4'] = (dr['ratings_4'] + dr['ratings_5']) / dr['ratings_count']\n",
    "\n",
    "\n",
    "# Estimate the probability\n",
    "probability_above_30_percent = (dr['proportion_above_4'] > 0.30).mean()\n",
    "\n",
    "print(probability_above_30_percent)\n",
    "\n",
    "print(f\"Estimated probability that a book has over 30% of ratings above 4: {probability_above_30_percent:.2%}\")\n",
    "pr = 0\n",
    "for i in range(0,len(dr)):\n",
    "    if dr['proportion_above_4'] [i] > 0.30:\n",
    "        pr += 1\n",
    "print('The probability to have over 30% of above 4 is',pr/(len(dr))*100,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d7a3a",
   "metadata": {},
   "source": [
    "* Estimate the probability that an author publishes a new book within two years from its last work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfad2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12344\\61683674.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dt['original_publication_date'] = pd.to_datetime(dt['original_publication_date'], errors='coerce')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12344\\61683674.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dt.sort_values(by=['author_id', 'original_publication_date'], ascending=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   title  author_id  \\\n",
      "8      The Hitchhiker's Guide to the Galaxy (Hitchhik...          4   \n",
      "11     The Hitchhiker's Guide to the Galaxy (Hitchhik...          4   \n",
      "13     The Hitchhiker's Guide to the Galaxy (Hitchhik...          4   \n",
      "2189   The Hitchhiker's Guide to the Galaxy (Hitchhik...          4   \n",
      "5469   The Hitchhiker's Guide to the Galaxy (Hitchhik...          4   \n",
      "...                                                  ...        ...   \n",
      "91729                                              Rawls   20915206   \n",
      "43245                        Momoko: A Novel of Betrayal   20915410   \n",
      "44858               From Sea to Shining Sea for Children   20927297   \n",
      "52776               The Light and the Glory for Children   20927297   \n",
      "35033  Shakespeare's Plutarch: The Lives Julius Caesa...   20979420   \n",
      "\n",
      "      publication_date original_publication_date  \n",
      "8                 2005                1979-10-12  \n",
      "11          2004-08-03                1979-10-12  \n",
      "13          2005-03-23                1979-10-12  \n",
      "2189        1983-03-03                1979-10-12  \n",
      "5469        2002-03-08                1979-10-12  \n",
      "...                ...                       ...  \n",
      "91729       2006-10-26                2006-10-01  \n",
      "43245       1994-06-01                1994-06-01  \n",
      "44858       1993-10-01                       NaT  \n",
      "52776       1992-12-01                       NaT  \n",
      "35033       1991-10-01                1991-10-01  \n",
      "\n",
      "[100084 rows x 4 columns]\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Overflow in int64 addition",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(dt)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate time gaps\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m dt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_gap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_publication_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdiff()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(dt)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Handle invalid (out-of-bounds) dates by replacing them with the previous work's publication date\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:5202\u001b[0m, in \u001b[0;36mGroupBy.diff\u001b[1;34m(self, periods, axis)\u001b[0m\n\u001b[0;32m   5199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(to_coerce):\n\u001b[0;32m   5200\u001b[0m         shifted \u001b[38;5;241m=\u001b[39m shifted\u001b[38;5;241m.\u001b[39mastype({c: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m to_coerce})\n\u001b[1;32m-> 5202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj \u001b[38;5;241m-\u001b[39m shifted\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39msub)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:5815\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   5814\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[1;32m-> 5815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\u001b[38;5;241m.\u001b[39mIndexOpsMixin\u001b[38;5;241m.\u001b[39m_arith_method(\u001b[38;5;28mself\u001b[39m, other, op)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:1381\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1378\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1381\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:275\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# NB: We assume that extract_array and ensure_wrapped_if_datetimelike\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m#  have already been called on `left` and `right`,\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m#  and `maybe_prepare_scalar_for_op` has already been called on `right`\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# We need to special-case datetime64/timedelta64 dtypes (e.g. because numpy\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# casts integer dtypes to timedelta64 when operating with timedelta64 - GH#22390)\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    269\u001b[0m     should_extension_dispatch(left, right)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, (Timedelta, BaseOffset, Timestamp))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;66;03m# because numexpr will fail on it, see GH#31457\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m op(left, right)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# error: Argument 2 to \"_bool_arith_check\" has incompatible type\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:1425\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1420\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_addsub_object_array(other, operator\u001b[38;5;241m.\u001b[39msub)\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(other_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1422\u001b[0m     other_dtype, DatetimeTZDtype\n\u001b[0;32m   1423\u001b[0m ):\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;66;03m# DatetimeIndex, ndarray[datetime64]\u001b[39;00m\n\u001b[1;32m-> 1425\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sub_datetime_arraylike(other)\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other_dtype, PeriodDtype):\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;66;03m# PeriodIndex\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sub_periodlike(other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:1116\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._sub_datetime_arraylike\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetimeArray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_matching_resos(other)\n\u001b[1;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sub_datetimelike(other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:1131\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._sub_datetimelike\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(new_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m other_i8, o_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_i8_values_and_mask(other)\n\u001b[1;32m-> 1131\u001b[0m res_values \u001b[38;5;241m=\u001b[39m checked_add_with_arr(\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masi8, \u001b[38;5;241m-\u001b[39mother_i8, arr_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_isnan, b_mask\u001b[38;5;241m=\u001b[39mo_mask\n\u001b[0;32m   1133\u001b[0m )\n\u001b[0;32m   1134\u001b[0m res_m8 \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimedelta64[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1136\u001b[0m new_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arithmetic_result_freq(other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1197\u001b[0m, in \u001b[0;36mchecked_add_with_arr\u001b[1;34m(arr, b, arr_mask, b_mask)\u001b[0m\n\u001b[0;32m   1192\u001b[0m     to_raise \u001b[38;5;241m=\u001b[39m ((i8max \u001b[38;5;241m-\u001b[39m b2[mask1] \u001b[38;5;241m<\u001b[39m arr[mask1]) \u001b[38;5;241m&\u001b[39m not_nan[mask1])\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1193\u001b[0m         (i8min \u001b[38;5;241m-\u001b[39m b2[mask2] \u001b[38;5;241m>\u001b[39m arr[mask2]) \u001b[38;5;241m&\u001b[39m not_nan[mask2]\n\u001b[0;32m   1194\u001b[0m     )\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_raise:\n\u001b[1;32m-> 1197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverflow in int64 addition\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1199\u001b[0m result \u001b[38;5;241m=\u001b[39m arr \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m b2_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOverflowError\u001b[0m: Overflow in int64 addition"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# we get the columsn we'll need\n",
    "dt = lb[['title','author_id','publication_date','original_publication_date']]\n",
    "\n",
    "# we change the  datetime to be able touse it more easily\n",
    "dt['original_publication_date'] = pd.to_datetime(dt['original_publication_date'], errors='coerce')\n",
    "\n",
    "dt.sort_values(by=['author_id', 'original_publication_date'], ascending=True, inplace=True)\n",
    "print(dt)\n",
    "# Calculate time gaps\n",
    "dt['time_gap'] = dt.groupby('author_id')['original_publication_date'].diff()\n",
    "print(dt)\n",
    "\n",
    "# Handle invalid (out-of-bounds) dates by replacing them with the previous work's publication date\n",
    "dt['original_publication_date'] = dt.groupby('author_id')['original_publication_date'].fillna(method='ffill')\n",
    "print(dt)\n",
    "\n",
    "# Filter the data\n",
    "filtered_data = dt.dropna(subset=['original_publication_date', 'time_gap'])\n",
    "print(filtered_data)\n",
    "filtered_data = filtered_data[filtered_data.groupby('author_id')['author_id'].transform('size') > 1]\n",
    "print(filtered_data)\n",
    "# Calculate the probability\n",
    "probability_within_two_years = dt.groupby('author_id')['time_gap'].apply(lambda x: (x <= timedelta(days=730)).any()).mean()\n",
    "\n",
    "print(f\"Estimated probability that an author publishes a new book within two years from their last work: {probability_within_two_years:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93436a6",
   "metadata": {},
   "source": [
    "* In the file list.json, you will find a peculiar list named \"The Worst Books of All Time.\" Estimate the probability of a book being included in this list, knowing it has more than 700 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "817d0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pathl = \"list.json\"\n",
    "ch_size = 100\n",
    "dfls = []\n",
    "\n",
    "# to import only some columns\n",
    "with open(file_pathl, \"r\") as filel:\n",
    "    rows_read = 0\n",
    "    for chl in pd.read_json(filel, lines = True, chunksize = ch_size):\n",
    "        chl = chl[[\"id\",\n",
    "\"title\",\n",
    "\"description\",\"description_html\",\"num_pages\",\n",
    "\"num_books\",\n",
    "\"num_voters\",\n",
    "\"created_date\",\n",
    "\"tags\",\n",
    "\"num_likes\",\n",
    "\"created_by\",\n",
    "\"num_comments\",\n",
    "\"books\"]]\n",
    "        dfls.append(chl)\n",
    "#          Update the count of rows read\n",
    "        rows_read += len(chl)\n",
    "        \n",
    "        # Check if we have read 10,000 rows, and if so, break the loop\n",
    "        if rows_read >= 100000:\n",
    "            break\n",
    "\n",
    "dfl = pd.concat(dfls, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cefbf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16729323308270677\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lb['num_pages'] = pd.to_numeric(lb['num_pages'], errors='coerce')  # Convert to integers and handle errors\n",
    "books_with_more_than_700_pages = lb[lb['num_pages'] > 700]\n",
    "\n",
    "# Step 2: Identify books in \"The Worst Books of All Time\" list\n",
    "# Assuming 'list' contains information about the list, extract the relevant book titles from it\n",
    "rs = dfl[dfl['title'] == \"The Worst Books of All Time\"]\n",
    "\n",
    "# Extract the titles of the books in the list\n",
    "\n",
    "# Create a new Series containing book dictionaries\n",
    "book_series = rs['books'].apply(lambda x: [item['title'] for item in x])\n",
    "\n",
    "# Create a flattened list of titles\n",
    "book_titles = [title for titles in book_series for title in titles]\n",
    "\n",
    "# Step 2: Identify books in the extracted list\n",
    "books_in_extracted_list = books_with_more_than_700_pages[books_with_more_than_700_pages['title'].isin(book_titles)]\n",
    "\n",
    "# Step 3: Calculate the probability\n",
    "total_books = len(books_with_more_than_700_pages)\n",
    "books_in_list_count = len(books_in_extracted_list)\n",
    "\n",
    "probability_in_list_given_700_pages = books_in_list_count / total_books\n",
    "\n",
    "print(probability_in_list_given_700_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27713386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of a book being on 'The Worst Books of All Time' list, given it has more than 700 pages: 0.19%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'lb' is a DataFrame containing book information\n",
    "# 'list' is a DataFrame containing list information\n",
    "\n",
    "# Step 1: Filter books with more than 700 pages from 'lb'\n",
    "# Convert 'num_pages' to integers and then filter\n",
    "lb['num_pages'] = pd.to_numeric(lb['num_pages'], errors='coerce')  # Convert to integers and handle errors\n",
    "books_with_more_than_700_pages = lb[lb['num_pages'] > 700]\n",
    "\n",
    "# Step 2: Identify books in \"The Worst Books of All Time\" list\n",
    "# Assuming 'list' contains information about the list, extract the relevant book titles from it\n",
    "worst_books_list = dfl[dfl['title'] == \"The Worst Books of All Time\"]\n",
    "\n",
    "# Step 3: Calculate the probability\n",
    "total_books = len(books_with_more_than_700_pages)\n",
    "books_in_worst_list_count = len(worst_books_list)\n",
    "\n",
    "probability_on_worst_list = books_in_worst_list_count / total_books\n",
    "\n",
    "print(f\"Estimated probability of a book being on 'The Worst Books of All Time' list, given it has more than 700 pages: {probability_on_worst_list:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0552a8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The events X and Y are independent.\n"
     ]
    }
   ],
   "source": [
    "# Calculate P(X), the probability of being in the list\n",
    "total_books = len(books_with_more_than_700_pages)  # Number of books with more than 700 pages\n",
    "books_in_worst_list_count = len(books_in_extracted_list)  # Number of books in the list\n",
    "\n",
    "P_X = books_in_worst_list_count / total_books\n",
    "\n",
    "# Calculate P(X|Y), the probability of being in the list given more than 700 pages\n",
    "total_books_with_more_than_700_pages = len(books_with_more_than_700_pages)  # Total books with more than 700 pages\n",
    "books_in_extracted_list_count = len(books_in_extracted_list)  # Number of books in the list with more than 700 pages\n",
    "P_X_given_Y = books_in_extracted_list_count / total_books_with_more_than_700_pages\n",
    "\n",
    "# Compare the probabilities\n",
    "if P_X_given_Y == P_X:\n",
    "    print(\"The events X and Y are independent.\")\n",
    "else:\n",
    "    print(\"The events X and Y are dependent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e0a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
