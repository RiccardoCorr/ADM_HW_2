{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ratings_count       1000 non-null   int64  \n",
      " 1   average_rating      1000 non-null   float64\n",
      " 2   text_reviews_count  1000 non-null   int64  \n",
      " 3   work_ids            1000 non-null   object \n",
      " 4   book_ids            1000 non-null   object \n",
      " 5   works_count         1000 non-null   int64  \n",
      " 6   id                  1000 non-null   int64  \n",
      " 7   name                1000 non-null   object \n",
      " 8   gender              1000 non-null   object \n",
      " 9   image_url           1000 non-null   object \n",
      " 10  about               1000 non-null   object \n",
      " 11  fans_count          1000 non-null   int64  \n",
      "dtypes: float64(1), int64(5), object(6)\n",
      "memory usage: 93.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(351767, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the first dataset \"author\" in a dataframe called authors to work on it\n",
    "\n",
    "file_path_authors = \".\\lighter_authors.json\"\n",
    "ch_size = 100\n",
    "dfs = []\n",
    "\n",
    "chunks = pd.read_json(file_path_authors, lines = True, chunksize=1000)\n",
    "\n",
    "# Read columns' names\n",
    "for chunk in chunks:\n",
    "    chunk.info()\n",
    "    break\n",
    "\n",
    "with open(file_path_authors, \"r\") as file:\n",
    "    for ch in pd.read_json(file, lines = True, chunksize = 1000):\n",
    "        dfs.append(ch)\n",
    "\n",
    "authors = pd.concat(dfs, ignore_index = True)\n",
    "\n",
    "authors.head(10)\n",
    "\n",
    "authors.shape # authors dataset has 351767 rows and 12 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_json(\"./lighter_books.json\", lines = True, chunksize=1000)\n",
    "\n",
    "good_columns  = []  # list with the columns' names\n",
    "\n",
    "for chunk in chunks:\n",
    "    good_columns  = chunk.columns.tolist()\n",
    "    break\n",
    "\n",
    "# print(good_columns)\n",
    "\n",
    "# Importing only the interesting columns from the dataset \"books\" to solve the exercise\n",
    "\n",
    "file_path_books = \"./lighter_books.json\"\n",
    "ch_size = 10000\n",
    "dfs = []\n",
    "\n",
    "with open(file_path_books, \"r\") as file:\n",
    "    for ch in pd.read_json(file, lines = True, chunksize = 1000):\n",
    "        ch = ch[[\"id\", \"title\", \"author_id\", \"author_name\", \"series_name\", \"series_id\", \"series_position\", \"format\",\n",
    "                \"original_publication_date\", \"work_id\", \"average_rating\", \"text_reviews_count\", \"language\",\"rating_dist\",\n",
    "                \"ratings_count\", \"publication_date\", \"edition_information\",\"publisher\",\"num_pages\"]]\n",
    "        # selecting only the useful columns\n",
    "        dfs.append(ch)\n",
    "\n",
    "\n",
    "books = pd.concat(dfs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_cleaned = books.drop_duplicates(subset=['title', 'author_id', 'series_id', 'series_position'], keep = \"first\")\n",
    "books_cleaned = books_cleaned.drop_duplicates(subset = ['work_id'])\n",
    "books_cleaned = books_cleaned[books_cleaned['title'] != \"\"]\n",
    "books_cleaned = books_cleaned[books_cleaned['author_id'] != \"\"]\n",
    "books_cleaned = books_cleaned[books_cleaned['author_name'] != \"NOT A BOOK\"]\n",
    "books_cleaned = books_cleaned[books_cleaned['author_name'] != \"Unknown\"]\n",
    "books_cleaned = books_cleaned[books_cleaned['author_name'] != \"Source Wikipedia\"]\n",
    "books_cleaned = books_cleaned[books_cleaned['author_name'] != \"Various\"]\n",
    "books_cleaned = books_cleaned[books_cleaned['author_name'] != \"Anonymous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7027431\n",
      "2459484\n"
     ]
    }
   ],
   "source": [
    "print(books.shape[0])\n",
    "print(books_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ3] Let’s have a historical look at the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a function that takes as input a year and returns as output the following information:\n",
    "\n",
    "    - The number of books published that year.\n",
    "\n",
    "    - The total number of pages written that year.\n",
    "\n",
    "    - The most prolific month of that year.\n",
    "\n",
    "    - The longest book written that year.\n",
    "\n",
    "* Use this function to build your data frame: the primary key will be a year, and the required information will be the attributes within the row. Finally, show the head and the tail of this new data frame considering the first ten years registered and the last ten years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = books_cleaned[[\"id\", \"title\", \"author_id\", \"author_name\", \"num_pages\", \"publication_date\", \"original_publication_date\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have two different date column then we have to select the one of them after examination of the dataset publication_date column more consistent than original_publication_date column but there are lots of null string ('') values in the both columns. \n",
    "\n",
    "For not losing the information, I have decided to get the date from original_publication_date column when publication_date has null string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>original_publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>The Hitchhiker's Guide to Lean: Lessons from t...</td>\n",
       "      <td>951394</td>\n",
       "      <td>Jamie Flinchbaugh</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2005-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>49</td>\n",
       "      <td>Bucaneer</td>\n",
       "      <td>17</td>\n",
       "      <td>Luther Butler</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2000-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>97</td>\n",
       "      <td>The Birthdays</td>\n",
       "      <td>52</td>\n",
       "      <td>Heidi Pitlor</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2007-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>116</td>\n",
       "      <td>The Dune Encyclopedia</td>\n",
       "      <td>1663914</td>\n",
       "      <td>Willis Everett McNelly</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1984-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>120</td>\n",
       "      <td>The Trust: The Private and Powerful Family beh...</td>\n",
       "      <td>985482</td>\n",
       "      <td>Susan E. Tifft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1999-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027391</th>\n",
       "      <td>25520909</td>\n",
       "      <td>The Hare and the Tortoise</td>\n",
       "      <td>7578490</td>\n",
       "      <td>Mark Anderson</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2015-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027409</th>\n",
       "      <td>25520953</td>\n",
       "      <td>Auch Sie können wieder jünger werden</td>\n",
       "      <td>250871</td>\n",
       "      <td>Norman W. Walker</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2015-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027412</th>\n",
       "      <td>25520964</td>\n",
       "      <td>Before That Night (Unfinished Love, #1)</td>\n",
       "      <td>7020583</td>\n",
       "      <td>Violet Duke</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2016-06-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027415</th>\n",
       "      <td>25520980</td>\n",
       "      <td>Renaissances</td>\n",
       "      <td>7493212</td>\n",
       "      <td>H.V. Gavriel</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2015-05-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027422</th>\n",
       "      <td>25521004</td>\n",
       "      <td>Love Vapors</td>\n",
       "      <td>893</td>\n",
       "      <td>Michael Allen</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2015-01-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456492 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                              title  \\\n",
       "12             15  The Hitchhiker's Guide to Lean: Lessons from t...   \n",
       "39             49                                           Bucaneer   \n",
       "81             97                                      The Birthdays   \n",
       "99            116                              The Dune Encyclopedia   \n",
       "103           120  The Trust: The Private and Powerful Family beh...   \n",
       "...           ...                                                ...   \n",
       "7027391  25520909                          The Hare and the Tortoise   \n",
       "7027409  25520953               Auch Sie können wieder jünger werden   \n",
       "7027412  25520964            Before That Night (Unfinished Love, #1)   \n",
       "7027415  25520980                                       Renaissances   \n",
       "7027422  25521004                                        Love Vapors   \n",
       "\n",
       "         author_id             author_name num_pages publication_date  \\\n",
       "12          951394       Jamie Flinchbaugh                              \n",
       "39              17           Luther Butler                              \n",
       "81              52            Heidi Pitlor                              \n",
       "99         1663914  Willis Everett McNelly                              \n",
       "103         985482          Susan E. Tifft                              \n",
       "...            ...                     ...       ...              ...   \n",
       "7027391    7578490           Mark Anderson                              \n",
       "7027409     250871        Norman W. Walker                              \n",
       "7027412    7020583             Violet Duke                              \n",
       "7027415    7493212            H.V. Gavriel                              \n",
       "7027422        893           Michael Allen                              \n",
       "\n",
       "        original_publication_date  \n",
       "12                     2005-12-01  \n",
       "39                     2000-10-18  \n",
       "81                     2007-01-01  \n",
       "99                        1984-06  \n",
       "103                    1999-12-30  \n",
       "...                           ...  \n",
       "7027391                2015-05-06  \n",
       "7027409                2015-05-11  \n",
       "7027412                2016-06-16  \n",
       "7027415                2015-05-08  \n",
       "7027422                2015-01-22  \n",
       "\n",
       "[456492 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books[(df_books['publication_date']=='') & (df_books['original_publication_date']!='')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\3961617708.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_books[\"date\"] = np.where(df_books['publication_date'] == '', df_books['original_publication_date'], df_books['publication_date'])\n"
     ]
    }
   ],
   "source": [
    "# Replace empty or null strings in column 'publication_date' with values from column 'original_publication_date'\n",
    "df_books[\"date\"] = np.where(df_books['publication_date'] == '', df_books['original_publication_date'], df_books['publication_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pages column has: 893130 empty string\n",
      "The median of the num_pages:  192.0\n",
      "The mean of the num_pages:  1604.0680823108953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\3367910375.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_books[\"num_pages\"].replace('', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Also, num_pages column has 893130 rows empty string\n",
    "print(\"num_pages column has:\", df_books[df_books['num_pages']==''].shape[0], \"empty string\")\n",
    "\n",
    "# Since a book has to be more than 1 page, I have decided to fill empty sting values with the median or mean of the num_pages column\n",
    "# But first we have to convert the type of the columns to float from string\n",
    "\n",
    "df_books[\"num_pages\"].replace('', np.nan, inplace=True)\n",
    "\n",
    "print(\"The median of the num_pages: \", df_books[\"num_pages\"].median())\n",
    "print(\"The mean of the num_pages: \", df_books[\"num_pages\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean of the num_pages is too big number, probably the dataset has outliers, that's why we choose the median to fill the empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty string count before filling up:  893130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\290903980.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_books[\"num_pages\"] = df_books[\"num_pages\"].fillna(df_books[\"num_pages\"].median())\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of empty string count before filling up: \", df_books[\"num_pages\"].isna().sum())\n",
    "\n",
    "df_books[\"num_pages\"] = df_books[\"num_pages\"].fillna(df_books[\"num_pages\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty string count after filling up with median:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of empty string count after filling up with median: \", df_books[\"num_pages\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert valid date strings to datetime, leaving others as NaT\n",
    "\n",
    "def custom_to_datetime(column, dataset):\n",
    "    column_formatted = []\n",
    "    for i in column:\n",
    "        if len(i) in [4, 7, 10]:\n",
    "            if len(i) == 4:\n",
    "                i += \"-01-01\"\n",
    "\n",
    "            if len(i) == 7:\n",
    "                i += \"-01\"\n",
    "            try:\n",
    "                format = \"%Y-%m-%d\"\n",
    "                timestamp_formattato = datetime.datetime.strptime(i, format)\n",
    "                column_formatted.append(timestamp_formattato)\n",
    "            except ValueError as ve:\n",
    "                column_formatted.append(pd.NaT)\n",
    "        else:\n",
    "            column_formatted.append(pd.NaT)\n",
    "    dataset['column_formatted'] = column_formatted\n",
    "    return dataset['column_formatted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\2138385396.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['column_formatted'] = column_formatted\n",
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\825771272.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_books['date_time'] = custom_to_datetime(df_books['date'], df_books)\n"
     ]
    }
   ],
   "source": [
    "# Apply the custom function to the 'dates' column\n",
    "df_books['date_time'] = custom_to_datetime(df_books['date'], df_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\146914065.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_books['month'] = df_books['date_time'].apply(lambda x: x.month if not pd.isna(x) else pd.NaT)\n",
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\146914065.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_books['year'] = df_books['date_time'].apply(lambda x: x.year if not pd.isna(x) else pd.NaT)\n"
     ]
    }
   ],
   "source": [
    "# Now we can get create month and year column by using date_time column\n",
    "\n",
    "df_books['month'] = df_books['date_time'].apply(lambda x: x.month if not pd.isna(x) else pd.NaT)\n",
    "df_books['year'] = df_books['date_time'].apply(lambda x: x.year if not pd.isna(x) else pd.NaT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>date_time</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2004-09-01 00:00:00</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>2003-11-01 00:00:00</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>2003-11-01 00:00:00</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2004-05-01 00:00:00</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>2002-09-28 00:00:00</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027412</th>\n",
       "      <td>6</td>\n",
       "      <td>2016-06-16 00:00:00</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027415</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-05-08 00:00:00</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027419</th>\n",
       "      <td>1</td>\n",
       "      <td>2006-01-01 00:00:00</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027422</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-22 00:00:00</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027430</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459484 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        month            date_time  year\n",
       "0           9  2004-09-01 00:00:00  2004\n",
       "1          11  2003-11-01 00:00:00  2003\n",
       "2          11  2003-11-01 00:00:00  2003\n",
       "3           5  2004-05-01 00:00:00  2004\n",
       "4           9  2002-09-28 00:00:00  2002\n",
       "...       ...                  ...   ...\n",
       "7027412     6  2016-06-16 00:00:00  2016\n",
       "7027415     5  2015-05-08 00:00:00  2015\n",
       "7027419     1  2006-01-01 00:00:00  2006\n",
       "7027422     1  2015-01-22 00:00:00  2015\n",
       "7027430     1  2015-01-01 00:00:00  2015\n",
       "\n",
       "[2459484 rows x 3 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books[['month', 'date_time', 'year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\116656058.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_books.dropna(axis=0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2276997"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since 182 rows has no date information, I will drop that rows from the dataset\n",
    "df_books.isna().sum()\n",
    "df_books.dropna(axis=0, inplace=True)\n",
    "df_books.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\2241613523.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_books[\"month\"] = df_books[\"month\"].astype(int)\n",
      "C:\\Users\\ricca\\AppData\\Local\\Temp\\ipykernel_33364\\2241613523.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_books[\"year\"] = df_books[\"year\"].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Converted float month and year column to int\n",
    "df_books[\"month\"] = df_books[\"month\"].astype(int)\n",
    "df_books[\"year\"] = df_books[\"year\"].astype(int)\n",
    "\n",
    "# Since dates after 2023 have not yet occurred, we are removing them from the data as they may be inaccurate data.\n",
    "df_books_year_filtered = df_books[df_books[\"year\"]<2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2276734, 12)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.shape\n",
    "df_books_year_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_details_by_year(df, year) :\n",
    "\n",
    "    df = df[df[\"year\"]==year]\n",
    "\n",
    "    #The number of books published that year\n",
    "    num_books = len(df)\n",
    "\n",
    "    #The total number of pages written that year.\n",
    "    num_pages = df[\"num_pages\"].sum()\n",
    "\n",
    "    #The most prolific month of that year.\n",
    "    prolific_month = df.groupby(by=[\"month\"])[\"id\"].count().sort_values(ascending=False).reset_index()[\"month\"].values[0]\n",
    "\n",
    "    #The longest book written that year.\n",
    "    longest_book = df[\"num_pages\"].max()\n",
    "\n",
    "\n",
    "    # Create a new dataframe to return with neccessary attributes\n",
    "    data = {'Year': [year], 'num_books': [num_books], 'num_pages': [num_pages], \n",
    "            'prolific_month': [prolific_month], 'longest_book': [longest_book]}\n",
    "\n",
    "    df_result = pd.DataFrame(data)\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1950, 2024)\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    result_df = pd.concat([result_df, get_book_details_by_year(df_books, year)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>num_books</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>prolific_month</th>\n",
       "      <th>longest_book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950</td>\n",
       "      <td>1340</td>\n",
       "      <td>299473.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1951</td>\n",
       "      <td>1263</td>\n",
       "      <td>288050.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5564.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1952</td>\n",
       "      <td>1329</td>\n",
       "      <td>308467.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1972.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1953</td>\n",
       "      <td>1363</td>\n",
       "      <td>311361.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1954</td>\n",
       "      <td>1449</td>\n",
       "      <td>324763.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4626.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  num_books  num_pages  prolific_month  longest_book\n",
       "0  1950       1340   299473.0               1        2550.0\n",
       "0  1951       1263   288050.0               1        5564.0\n",
       "0  1952       1329   308467.0               1        1972.0\n",
       "0  1953       1363   311361.0               1        2922.0\n",
       "0  1954       1449   324763.0               1        4626.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.sort_values(by=\"Year\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>num_books</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>prolific_month</th>\n",
       "      <th>longest_book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>709</td>\n",
       "      <td>181140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>480</td>\n",
       "      <td>122191.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>108</td>\n",
       "      <td>28306.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>1</td>\n",
       "      <td>463.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  num_books  num_pages  prolific_month  longest_book\n",
       "0  2019        709   181140.0               1        1113.0\n",
       "0  2020        480   122191.0               1        3260.0\n",
       "0  2021        108    28306.0               1        1300.0\n",
       "0  2022         12     2756.0               1         500.0\n",
       "0  2023          7     1673.0               1         463.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.sort_values(by=\"Year\").tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ask ChatGPT or any other LLM chatbot tool to implement this function and compare your work with the one the bot gave you as an answer. Does the chatbot implementation work? Please test it out and verify the correctness of the implementation, explaining the process you followed to prove it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>num_books</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>prolific_month</th>\n",
       "      <th>longest_book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>44189</td>\n",
       "      <td>1.025312e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.800000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>44912</td>\n",
       "      <td>1.057911e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.397400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>47513</td>\n",
       "      <td>1.135575e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.081500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>52069</td>\n",
       "      <td>1.232873e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.988800e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>57095</td>\n",
       "      <td>1.332490e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.113400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>63689</td>\n",
       "      <td>1.472180e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.131400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>89353</td>\n",
       "      <td>2.276263e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.398700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>80329</td>\n",
       "      <td>1.875588e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.134900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>80574</td>\n",
       "      <td>1.833703e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.134000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>88936</td>\n",
       "      <td>2.166996e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.147484e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>108</td>\n",
       "      <td>2.830600e+04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.300000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>2.756000e+03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>1.673000e+03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.630000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2026</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2027</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2029</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2030</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  num_books     num_pages  prolific_month  longest_book\n",
       "0   2000      44189  1.025312e+07             1.0  7.800000e+03\n",
       "1   2001      44912  1.057911e+07             1.0  1.397400e+04\n",
       "2   2002      47513  1.135575e+07             1.0  4.081500e+04\n",
       "3   2003      52069  1.232873e+07             1.0  1.988800e+04\n",
       "4   2004      57095  1.332490e+07             1.0  3.113400e+04\n",
       "5   2005      63689  1.472180e+07             1.0  1.131400e+04\n",
       "6   2006      89353  2.276263e+07             1.0  6.398700e+04\n",
       "7   2007      80329  1.875588e+07             1.0  4.134900e+04\n",
       "8   2008      80574  1.833703e+07             1.0  6.134000e+04\n",
       "9   2009      88936  2.166996e+09             1.0  2.147484e+09\n",
       "21  2021        108  2.830600e+04             1.0  1.300000e+03\n",
       "22  2022         12  2.756000e+03             1.0  5.000000e+02\n",
       "23  2023          7  1.673000e+03             1.0  4.630000e+02\n",
       "24  2024          0  0.000000e+00             NaN           NaN\n",
       "25  2025          0  0.000000e+00             NaN           NaN\n",
       "26  2026          0  0.000000e+00             NaN           NaN\n",
       "27  2027          0  0.000000e+00             NaN           NaN\n",
       "28  2028          0  0.000000e+00             NaN           NaN\n",
       "29  2029          0  0.000000e+00             NaN           NaN\n",
       "30  2030          0  0.000000e+00             NaN           NaN"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_book_details_by_year(df, year):\n",
    "    year_df = df[df[\"year\"] == year]\n",
    "\n",
    "    if year_df.empty:\n",
    "        # Handle the case when there are no records for the given year\n",
    "        return {\n",
    "            'Year': year,\n",
    "            'num_books': 0,\n",
    "            'num_pages': 0,\n",
    "            'prolific_month': None,\n",
    "            'longest_book': None\n",
    "        }\n",
    "\n",
    "    num_books = len(year_df)\n",
    "    num_pages = year_df[\"num_pages\"].sum()\n",
    "    prolific_month = year_df.groupby(by=[\"month\"])[\"id\"].count().sort_values(ascending=False).reset_index()[\"month\"].values[0]\n",
    "    longest_book = year_df[\"num_pages\"].max()\n",
    "\n",
    "    return {\n",
    "        'Year': year,\n",
    "        'num_books': num_books,\n",
    "        'num_pages': num_pages,\n",
    "        'prolific_month': prolific_month,\n",
    "        'longest_book': longest_book\n",
    "    }\n",
    "\n",
    "\n",
    "years = range(2000, 2031)  # Consider the first 31 years as an example\n",
    "\n",
    "results = [get_book_details_by_year(df_books_year_filtered, year) for year in years]\n",
    "df_result = pd.DataFrame(results)\n",
    "\n",
    "# Display the head and tail of the DataFrame\n",
    "head_and_tail = pd.concat([df_result.head(10), df_result.tail(10)])\n",
    "head_and_tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatgpt Answer doesn't work properly. It returns first and last ten years but the data is not correct that has a lot of NaN values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ4] Quirks questions about consistency. In most cases, we will not have a consistent dataset, and the one we are dealing with is no exception. So, let's enhance our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You should be sure there are no eponymous (different authors who have precisely the same name) in the author's dataset. Is it true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there any duplicated author in the dataset: True\n",
      "Duplicated author name list:  ['Joseph Fink', 'Boris Zakhoder', 'James Kent', 'Caroline Miller', 'Peter  Marshall', 'William Messner-Loebs', 'Jorge Molina', 'Paul Graham', 'Q. Hayashida', '小野不由美', 'John  Mole', 'James C.L. Carson', 'Mike   Lee', 'Dimitar Dimov', 'Peter      Marshall', 'Yordan Yovkov', 'Cicerón', 'Catherine   Jones', 'Hildegard von Bingen', 'Peter  Davies', 'Robert W. Sullivan IV', 'David Yates', 'Peter    Green', 'Julie  Campbell', 'George  Franklin', 'M.K. Graff', 'Paul      Davies', 'Peter Green', 'محمد نجيب', 'Peter King', 'Jackson Butch Guice', 'Katherine Mercurio Gotthardt', 'Erin  Bedford', 'Martin    Shaw', 'Christopher Phillips', 'Chris Lynch', 'David  Nelson']\n"
     ]
    }
   ],
   "source": [
    "# check any duplicated author name\n",
    "print(\"Is there any duplicated author in the dataset:\", authors.name.duplicated().any())\n",
    "\n",
    "df_author_name_duplicated = authors.name.value_counts().reset_index()\n",
    "duplicated_author_name_list = list(df_author_name_duplicated[df_author_name_duplicated[\"count\"]>1][\"name\"].values)\n",
    "\n",
    "print(\"Duplicated author name list: \", duplicated_author_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a function that, given a list of author_id, outputs a dictionary where each author_id is a key, and the related value is a list \n",
    "with the names of all the books the author has written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.rename(columns={'id': 'author_id'}, inplace=True)\n",
    "df_authors_books = authors.merge(books_cleaned, on='author_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count_x</th>\n",
       "      <th>average_rating_x</th>\n",
       "      <th>text_reviews_count_x</th>\n",
       "      <th>work_ids</th>\n",
       "      <th>book_ids</th>\n",
       "      <th>works_count</th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>image_url</th>\n",
       "      <th>...</th>\n",
       "      <th>work_id</th>\n",
       "      <th>average_rating_y</th>\n",
       "      <th>text_reviews_count_y</th>\n",
       "      <th>language</th>\n",
       "      <th>rating_dist</th>\n",
       "      <th>ratings_count_y</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>publisher</th>\n",
       "      <th>num_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2862064</td>\n",
       "      <td>4.19</td>\n",
       "      <td>62681</td>\n",
       "      <td>[3078186, 135328, 1877624, 74123, 3078120, 104...</td>\n",
       "      <td>[386162, 13, 8695, 8694, 6091075, 365, 569429,...</td>\n",
       "      <td>106</td>\n",
       "      <td>4</td>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/159137433...</td>\n",
       "      <td>...</td>\n",
       "      <td>3078186.0</td>\n",
       "      <td>4.22</td>\n",
       "      <td>31417.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>5:753962|4:436665|3:206876|2:60681|1:32955|tot...</td>\n",
       "      <td>1491139.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>Movie Tie-In Edition</td>\n",
       "      <td>Del Rey Books</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2862064</td>\n",
       "      <td>4.19</td>\n",
       "      <td>62681</td>\n",
       "      <td>[3078186, 135328, 1877624, 74123, 3078120, 104...</td>\n",
       "      <td>[386162, 13, 8695, 8694, 6091075, 365, 569429,...</td>\n",
       "      <td>106</td>\n",
       "      <td>4</td>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/159137433...</td>\n",
       "      <td>...</td>\n",
       "      <td>135328.0</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5429.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>5:167705|4:81013|3:32283|2:8665|1:4360|total:2...</td>\n",
       "      <td>294026.0</td>\n",
       "      <td>2005-11-01</td>\n",
       "      <td></td>\n",
       "      <td>Gramercy Books</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2862064</td>\n",
       "      <td>4.19</td>\n",
       "      <td>62681</td>\n",
       "      <td>[3078186, 135328, 1877624, 74123, 3078120, 104...</td>\n",
       "      <td>[386162, 13, 8695, 8694, 6091075, 365, 569429,...</td>\n",
       "      <td>106</td>\n",
       "      <td>4</td>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/159137433...</td>\n",
       "      <td>...</td>\n",
       "      <td>69899216.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>33.0</td>\n",
       "      <td></td>\n",
       "      <td>5:413|4:234|3:106|2:13|1:4|total:770</td>\n",
       "      <td>770.0</td>\n",
       "      <td>2005-10-25</td>\n",
       "      <td>Dramatization edition; unabridged</td>\n",
       "      <td>BBC Audiobooks</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings_count_x  average_rating_x  text_reviews_count_x  \\\n",
       "0          2862064              4.19                 62681   \n",
       "1          2862064              4.19                 62681   \n",
       "2          2862064              4.19                 62681   \n",
       "\n",
       "                                            work_ids  \\\n",
       "0  [3078186, 135328, 1877624, 74123, 3078120, 104...   \n",
       "1  [3078186, 135328, 1877624, 74123, 3078120, 104...   \n",
       "2  [3078186, 135328, 1877624, 74123, 3078120, 104...   \n",
       "\n",
       "                                            book_ids  works_count  author_id  \\\n",
       "0  [386162, 13, 8695, 8694, 6091075, 365, 569429,...          106          4   \n",
       "1  [386162, 13, 8695, 8694, 6091075, 365, 569429,...          106          4   \n",
       "2  [386162, 13, 8695, 8694, 6091075, 365, 569429,...          106          4   \n",
       "\n",
       "            name gender                                          image_url  \\\n",
       "0  Douglas Adams   male  https://images.gr-assets.com/authors/159137433...   \n",
       "1  Douglas Adams   male  https://images.gr-assets.com/authors/159137433...   \n",
       "2  Douglas Adams   male  https://images.gr-assets.com/authors/159137433...   \n",
       "\n",
       "   ...     work_id  average_rating_y  text_reviews_count_y language  \\\n",
       "0  ...   3078186.0              4.22               31417.0      eng   \n",
       "1  ...    135328.0              4.36                5429.0      eng   \n",
       "2  ...  69899216.0              4.35                  33.0            \n",
       "\n",
       "                                         rating_dist ratings_count_y  \\\n",
       "0  5:753962|4:436665|3:206876|2:60681|1:32955|tot...       1491139.0   \n",
       "1  5:167705|4:81013|3:32283|2:8665|1:4360|total:2...        294026.0   \n",
       "2               5:413|4:234|3:106|2:13|1:4|total:770           770.0   \n",
       "\n",
       "  publication_date                edition_information       publisher  \\\n",
       "0             2005               Movie Tie-In Edition   Del Rey Books   \n",
       "1       2005-11-01                                     Gramercy Books   \n",
       "2       2005-10-25  Dramatization edition; unabridged  BBC Audiobooks   \n",
       "\n",
       "  num_pages  \n",
       "0       216  \n",
       "1       815  \n",
       "2            \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_authors_books.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: [\"The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1)\", \"The Ultimate Hitchhiker's Guide: Five Complete Novels and One Story (Hitchhiker's Guide to the Galaxy, #1-5)\", \"The Hitchhiker's Guide to the Galaxy: Quandary Phase (Hitchhiker's Guide: Radio Play, #4)\", \"The Hitchhiker's Guide to the Galaxy: Quintessential Phase (Hitchhiker's Guide: Radio Play, #5)\", 'The Long Dark Tea-Time of the Soul (Dirk Gently, #2)', \"Dirk Gently's Holistic Detective Agency (Dirk Gently, #1)\", 'The Salmon of Doubt (Dirk Gently, #3)', \"Mostly Harmless (Hitchhiker's Guide to the Galaxy, #5)\", \"Life, the Universe and Everything (Hitchhiker's Guide, #3)\", 'The Deeper Meaning of Liff', \"The Restaurant at the End of the Universe (Hitchhiker's Guide to the Galaxy, #2)\", 'Last Chance to See', \"So Long, and Thanks for All the Fish (Hitchhiker's Guide to the Galaxy, #4)\", \"Mostly Brilliant (Hitchhiker's Guide, #1-5)\", \"Guide to the Hitchhiker's Guide to the Galaxy\", \"Two Complete Novels: Dirk Gently's Holistic Detective Agency / The Long Dark Tea-time of the Soul (Dirk Gently #1-2)\", \"Hitch-hiker's Guide to the Galaxy: The Original Radio Scripts\", \"The Illustrated Hitchhiker's Guide To The Galaxy\", \"The Hitchhiker's Guide to the Galaxy Radio Scripts: Tertiary, Quandary & Quintessential Phases\", \"The More Than Complete Hitchhiker's Guide (Hitchhiker's Guide, #1-4 + short story)\", \"The Hitchhiker's Guide to the Galaxy Omnibus (Hitchhiker's Guide, #1-4)\", 'The Meaning of Liff (Meaning of Liff, #1)', 'Douglas Adams Live in Concert', \"Douglas Adams at the BBC: A Celebration of the Author's Life and Work\", \"The Hitchhiker's Guide to the Galaxy: The Tertiary Phase (Hitchhiker's Guide: Radio Play, #3)\", \"The Hitchhiker's Trilogy\", 'The Utterly Utterly Merry Comic Relief Christmas Book', 'Not 1982', \"The Hitch Hiker's Guide To The Galaxy: Primary Phase\", \"The Hitchhiker's Guide to the Galaxy: Primary & Secondary Phase\", 'Per Anhalter Durch Die Galaxis 2 Hörspiel', 'Per Anhalter durch die Galaxis/Das Restaurant am Ende des Universums (Per Anhalter durch die Galaxis, #1-2)', 'Der junge Zaphod geht auf Nummer Sicher', 'Hitchhikers Guide: 2 Secondary Spec.(4 Cd', \"Per Anhalter ins All 7 - 9: Das Sofa auf Lord's Cricket Ground\", 'Per Anhalter ins All 1/2: Schluß mit der Erde / Roboter und Doppelkopf', 'Per Anhalter ins All 5/6: Ein Tango am Ende der Welt / Die Erde hat uns wieder', \"The Lost Chapters of the Hitchhiker's Guide to the Galaxy\", 'The Private Life of Genghis Khan', 'Elimäen kootut tarkoitukset', 'Doctor Who: City of Death', 'Vodič kroz galaksiju za autostopere 2 (Vodič kroz galaksiju za autostopere #4-5)', 'Keliautojo kosmostopu vadovas po galaktiką', \"Three book set, includes The Salmon of Doubt, Dirk Gently's Holistic Detective Agency, and The Long Dark Tea-Time of the Soul\", 'Douglas Adams: The Ultimate Collection'], 7: ['A Short History of Nearly Everything', \"Bill Bryson's African Diary\", \"Bryson's Dictionary of Troublesome Words: A Writer's Guide to Getting It Right\", 'In a Sunburned Country', \"I'm a Stranger Here Myself: Notes on Returning to America After Twenty Years Away\", 'The Lost Continent: Travels in Small Town America', 'Neither Here nor There: Travels in Europe', 'Notes from a Small Island', 'The Mother Tongue: English and How It Got That Way', 'A Walk in the Woods: Rediscovering America on the Appalachian Trail', 'The Life and Times of the Thunderbolt Kid', 'Made in America: An Informal History of the English Language in the United States', \"Bill Bryson Collector's Edition: Notes from a Small Island / Neither Here Nor There / I'm a Stranger Here Myself\", 'Bill Bryson: The Complete Notes', 'Walkabout: A Walk in the Woods & Down Under', 'The Palace Under the Alps, and Over 200 Other Unusual, Unspoiled, and Infrequently Visited Spots in 16 European Countries', 'Journeys in English', 'The English Landscape: Its Character and Diversity', 'The Best American Travel Writing 2000', 'Shakespeare: The World as Stage', 'The Lost Continent & Neither Here Nor There', \"Bryson's Dictionary for Writers and Editors\", 'A Really Short History of Nearly Everything', 'Bill Bryson Box Set:  Three Vols. A Walk In The Woods, Notes From A Big Country, Notes From A Small Island', 'Icons of England', 'The Road Less Travelled: 1,000 Amazing Places off the Tourist Trail', 'Seeing Further: Ideas, Endeavours, Discoveries and Disputes — The Story of Science Through 350 Years of the Royal Society', 'At Home: A Short History of Private Life', 'Bill Bryson Series Collection: Troublesome Words, Mother Tongue: The Story Of The English Language, Icons Of England, Made In America & At Home', 'One Summer: America, 1927', 'Bill Bryson for The Love Hearts Appeal', 'A Short (Natural) History of the Garden', \"Still Me/One of Ours: Timothy McVeigh & the Oklahoma City Bombing/A Walk in the Woods/Dr Spock: An American Life (Reader's Digest Today's Best Nonfiction, Volume 51: 1998)\", 'The Road to Little Dribbling: More Notes From a Small Island'], 100: ['The Book of Evidence', 'Science Rules: A Historical Introduction to Scientific Methods', 'Concepts of Science: A Philosophical Analysis', 'The Nature of Explanation', 'Particles and Waves: Historical Essays in the Philosophy of Science', 'Observation, Experiment, and Hypothesis in Modern Physical Science', 'Law And Explanation:An Essay In The Philosophy Of Science.', 'Scientific Methods: Conceptual and Historical Problems', 'The Legacy of Logical Positivism in the Philosophy of Science', 'Scientific Evidence: Philosophical Theories and Applications', 'The Concept Of Evidence', 'Evidence, Explanation, and Realism: Essays in Philosophy of Science', 'Explanation: Papers and Discussions', 'Explanation', 'Evidence and Method: Scientific Strategies of Isaac Newton and James Clerk Maxwell'], 7133040: ['Stories from Hadraemscapen (Birth of a New Dawn #1, A Time of Hope #2, Darkness Settles #3)']}\n"
     ]
    }
   ],
   "source": [
    "def get_books_by_author(df, author_ids):\n",
    "    \n",
    "    author_books_dict = {}\n",
    "\n",
    "    for author_id in author_ids:\n",
    "        books = df[df['author_id'] == author_id]['title'].unique().tolist()\n",
    "        author_books_dict[author_id] = books\n",
    "\n",
    "    return author_books_dict\n",
    "\n",
    "# List of author_ids for which you want to find books\n",
    "author_ids_to_lookup = [4, 7, 100, 7133040]\n",
    "\n",
    "result = get_books_by_author(df_authors_books, author_ids_to_lookup)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the longest book title among the books of the top 20 authors regarding their average rating? Is it the longest book title overall?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating_x</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1954764</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178264</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19516511</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6873873</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7133040</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363742</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7369741</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15879402</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641231</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159944</th>\n",
       "      <td>-31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351767 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           average_rating_x\n",
       "author_id                  \n",
       "1954764                 5.0\n",
       "8178264                 5.0\n",
       "19516511                5.0\n",
       "6873873                 5.0\n",
       "7133040                 5.0\n",
       "...                     ...\n",
       "1363742                 0.0\n",
       "7369741                 0.0\n",
       "15879402                0.0\n",
       "6641231                 0.0\n",
       "7159944               -31.0\n",
       "\n",
       "[351767 rows x 1 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# While checking the average author scores to find the top 20 authors, I see that more than 20 authors received full scores, \n",
    "# so I add all authors who received 5 points to the best authors list.\n",
    "\n",
    "df_authors_books.groupby(by='author_id').agg({'average_rating_x': 'first'}).sort_values(by='average_rating_x', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors_books['title_length'] = df_authors_books['title'].apply(lambda x: len(x) if isinstance(x, str) else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum title length is:  255.0\n"
     ]
    }
   ],
   "source": [
    "max_title_len = df_authors_books[df_authors_books[\"average_rating_x\"]==5][\"title_length\"].max()\n",
    "print(\"Maximum title length is: \", max_title_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13801</th>\n",
       "      <td>632</td>\n",
       "      <td>Lon Milo DuQuette</td>\n",
       "      <td>The Grand and Noble Order of Button Busters: A...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19888</th>\n",
       "      <td>947</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>Shakespearean extracts from Edward Pudsey's bo...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26712</th>\n",
       "      <td>1438</td>\n",
       "      <td>Walt Whitman</td>\n",
       "      <td>The gathering of the forces; editorials, essay...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>1480</td>\n",
       "      <td>John  Adams</td>\n",
       "      <td>Biographical Sketches of Distinguished America...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30426</th>\n",
       "      <td>1673</td>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>A Manual of Parliamentary Practice, Composed O...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535269</th>\n",
       "      <td>15878806</td>\n",
       "      <td>Henry  Moore</td>\n",
       "      <td>Instructions for Preparing Abstracts of Titles...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535469</th>\n",
       "      <td>15885133</td>\n",
       "      <td>Robert Pocock</td>\n",
       "      <td>Pocock's everlasting songster, containing a se...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536148</th>\n",
       "      <td>15912139</td>\n",
       "      <td>Samuel  Clarke</td>\n",
       "      <td>A Collection of the Promises of Scripture, Und...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536940</th>\n",
       "      <td>15948238</td>\n",
       "      <td>Charles  Owen</td>\n",
       "      <td>Plain reasons, I. For dissenting from the comm...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542146</th>\n",
       "      <td>16171613</td>\n",
       "      <td>John E.  Bennett</td>\n",
       "      <td>Mandell, Douglas y Bennet Enfermedades Infecci...</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>713 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author_id                 name  \\\n",
       "13801          632    Lon Milo DuQuette   \n",
       "19888          947  William Shakespeare   \n",
       "26712         1438         Walt Whitman   \n",
       "27996         1480          John  Adams   \n",
       "30426         1673     Thomas Jefferson   \n",
       "...            ...                  ...   \n",
       "2535269   15878806         Henry  Moore   \n",
       "2535469   15885133        Robert Pocock   \n",
       "2536148   15912139       Samuel  Clarke   \n",
       "2536940   15948238        Charles  Owen   \n",
       "2542146   16171613     John E.  Bennett   \n",
       "\n",
       "                                                     title  title_length  \n",
       "13801    The Grand and Noble Order of Button Busters: A...         255.0  \n",
       "19888    Shakespearean extracts from Edward Pudsey's bo...         255.0  \n",
       "26712    The gathering of the forces; editorials, essay...         255.0  \n",
       "27996    Biographical Sketches of Distinguished America...         255.0  \n",
       "30426    A Manual of Parliamentary Practice, Composed O...         255.0  \n",
       "...                                                    ...           ...  \n",
       "2535269  Instructions for Preparing Abstracts of Titles...         255.0  \n",
       "2535469  Pocock's everlasting songster, containing a se...         255.0  \n",
       "2536148  A Collection of the Promises of Scripture, Und...         255.0  \n",
       "2536940  Plain reasons, I. For dissenting from the comm...         255.0  \n",
       "2542146  Mandell, Douglas y Bennet Enfermedades Infecci...         255.0  \n",
       "\n",
       "[713 rows x 4 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 713 rows has a maximum title length (255)\n",
    "df_authors_books[df_authors_books[\"title_length\"] == max_title_len][[\"author_id\", \"name\", \"title\", \"title_length\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the shortest overall book title in the dataset? If you find something strange, provide a comment on what happened and an alternative answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count_x</th>\n",
       "      <th>average_rating_x</th>\n",
       "      <th>text_reviews_count_x</th>\n",
       "      <th>work_ids</th>\n",
       "      <th>book_ids</th>\n",
       "      <th>works_count</th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>image_url</th>\n",
       "      <th>...</th>\n",
       "      <th>average_rating_y</th>\n",
       "      <th>text_reviews_count_y</th>\n",
       "      <th>language</th>\n",
       "      <th>rating_dist</th>\n",
       "      <th>ratings_count_y</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>publisher</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ratings_count_x, average_rating_x, text_reviews_count_x, work_ids, book_ids, works_count, author_id, name, gender, image_url, about, fans_count, id, title, author_name, series_name, series_id, series_position, format, original_publication_date, work_id, average_rating_y, text_reviews_count_y, language, rating_dist, ratings_count_y, publication_date, edition_information, publisher, num_pages, title_length]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 31 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There aren't rows without any name\n",
    "print(df_authors_books[df_authors_books[\"title_length\"]==0].shape)\n",
    "\n",
    "df_authors_books[df_authors_books[\"title_length\"]==0].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count_x</th>\n",
       "      <th>average_rating_x</th>\n",
       "      <th>text_reviews_count_x</th>\n",
       "      <th>work_ids</th>\n",
       "      <th>book_ids</th>\n",
       "      <th>works_count</th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>image_url</th>\n",
       "      <th>...</th>\n",
       "      <th>average_rating_y</th>\n",
       "      <th>text_reviews_count_y</th>\n",
       "      <th>language</th>\n",
       "      <th>rating_dist</th>\n",
       "      <th>ratings_count_y</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>publisher</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22769</th>\n",
       "      <td>47012</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1135</td>\n",
       "      <td>[1283844, 366428, 382424, 58136682, 47615, 968...</td>\n",
       "      <td>[31818, 376579, 130940, 36436103, 48672, 10047...</td>\n",
       "      <td>218</td>\n",
       "      <td>1203</td>\n",
       "      <td>Andy Warhol</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/120656521...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.39</td>\n",
       "      <td>22.0</td>\n",
       "      <td></td>\n",
       "      <td>5:61|4:48|3:70|2:27|1:26|total:232</td>\n",
       "      <td>232.0</td>\n",
       "      <td>1998-02-17</td>\n",
       "      <td></td>\n",
       "      <td>Grove Press</td>\n",
       "      <td>384</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27546</th>\n",
       "      <td>15836</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1133</td>\n",
       "      <td>[537899, 1810799, 1192753, 100524, 1817057, 56...</td>\n",
       "      <td>[550655, 919995, 104260, 104262, 919987, 15247...</td>\n",
       "      <td>903</td>\n",
       "      <td>1464</td>\n",
       "      <td>George Sand</td>\n",
       "      <td>female</td>\n",
       "      <td>https://images.gr-assets.com/authors/134111243...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>5:1|4:1|3:2|2:0|1:0|total:4</td>\n",
       "      <td>4.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48869</th>\n",
       "      <td>68829</td>\n",
       "      <td>3.70</td>\n",
       "      <td>6923</td>\n",
       "      <td>[502278, 6512514, 14916864, 1874015, 64658630,...</td>\n",
       "      <td>[514313, 6326920, 10021420, 86161, 41433284, 9...</td>\n",
       "      <td>34</td>\n",
       "      <td>2850</td>\n",
       "      <td>Ben Mezrich</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/140544550...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.59</td>\n",
       "      <td>39.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>5:66|4:103|3:84|2:36|1:11|total:300</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td></td>\n",
       "      <td>FG Press</td>\n",
       "      <td>160</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68276</th>\n",
       "      <td>3392332</td>\n",
       "      <td>4.12</td>\n",
       "      <td>97735</td>\n",
       "      <td>[1015554, 2765786, 1249788, 2379261, 105046, 8...</td>\n",
       "      <td>[39988, 6310, 6319, 6689, 6327, 31456, 6693, 6...</td>\n",
       "      <td>825</td>\n",
       "      <td>4273</td>\n",
       "      <td>Roald Dahl</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/131155490...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>5:3|4:0|3:0|2:1|1:0|total:4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-05-30</td>\n",
       "      <td></td>\n",
       "      <td>강</td>\n",
       "      <td>353</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74646</th>\n",
       "      <td>43540</td>\n",
       "      <td>4.26</td>\n",
       "      <td>1712</td>\n",
       "      <td>[273861, 23929637, 18280, 273864, 6229228, 273...</td>\n",
       "      <td>[282308, 17295645, 16602, 282311, 6053428, 282...</td>\n",
       "      <td>165</td>\n",
       "      <td>4743</td>\n",
       "      <td>Yoshitaka Amano</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/124492114...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ger</td>\n",
       "      <td>5:1|4:0|3:0|2:0|1:0|total:1</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498568</th>\n",
       "      <td>163</td>\n",
       "      <td>3.96</td>\n",
       "      <td>7</td>\n",
       "      <td>[2426727, 21505358, 43597609, 15313686, 407875...</td>\n",
       "      <td>[17900446, 15786351, 23997091, 10409565, 21469...</td>\n",
       "      <td>36</td>\n",
       "      <td>14381327</td>\n",
       "      <td>Dumitru Radu Popescu</td>\n",
       "      <td>male</td>\n",
       "      <td>https://s.gr-assets.com/assets/nophoto/user/m_...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rum</td>\n",
       "      <td>5:6|4:4|3:2|2:0|1:0|total:12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Gramar</td>\n",
       "      <td>316</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502247</th>\n",
       "      <td>2</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "      <td>[64629537, 52890761, 56156380, 24270915, 59343...</td>\n",
       "      <td>[41410287, 32262726, 34901031, 17418023, 37709...</td>\n",
       "      <td>22</td>\n",
       "      <td>14539438</td>\n",
       "      <td>Philippe Dautzenberg</td>\n",
       "      <td></td>\n",
       "      <td>https://s.gr-assets.com/assets/nophoto/user/u_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>5:0|4:0|3:0|2:0|1:0|total:0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525292</th>\n",
       "      <td>8</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2</td>\n",
       "      <td>[16920761, 47026229, 66214585, 19975563, 69519...</td>\n",
       "      <td>[11958502, 26976387, 42502486, 14333723, 44849...</td>\n",
       "      <td>33</td>\n",
       "      <td>15379169</td>\n",
       "      <td>Thomas Balch</td>\n",
       "      <td></td>\n",
       "      <td>https://s.gr-assets.com/assets/nophoto/user/u_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>5:0|4:0|3:0|2:0|1:0|total:0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525875</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>[68604288, 64782294, 86346988, 48600290, 17282...</td>\n",
       "      <td>[44111406, 41509071, 30767954, 28462303, 12305...</td>\n",
       "      <td>8</td>\n",
       "      <td>15405966</td>\n",
       "      <td>Stanisław II August</td>\n",
       "      <td>female</td>\n",
       "      <td>https://images.gr-assets.com/authors/160040463...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>5:0|4:0|3:0|2:0|1:0|total:0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547242</th>\n",
       "      <td>12</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3</td>\n",
       "      <td>[15045560, 86179454, 18444525]</td>\n",
       "      <td>[10147283, 55265814, 13244210]</td>\n",
       "      <td>3</td>\n",
       "      <td>16428797</td>\n",
       "      <td>Andrea        Ferrari</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/148684958...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ita</td>\n",
       "      <td>5:1|4:2|3:3|2:1|1:0|total:7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2008-03-28</td>\n",
       "      <td>Le vele series</td>\n",
       "      <td>Fazi Editore, Roma</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ratings_count_x  average_rating_x  text_reviews_count_x  \\\n",
       "22769              47012              3.81                  1135   \n",
       "27546              15836              3.71                  1133   \n",
       "48869              68829              3.70                  6923   \n",
       "68276            3392332              4.12                 97735   \n",
       "74646              43540              4.26                  1712   \n",
       "...                  ...               ...                   ...   \n",
       "2498568              163              3.96                     7   \n",
       "2502247                2              5.00                     0   \n",
       "2525292                8              3.62                     2   \n",
       "2525875                0              0.00                     0   \n",
       "2547242               12              3.25                     3   \n",
       "\n",
       "                                                  work_ids  \\\n",
       "22769    [1283844, 366428, 382424, 58136682, 47615, 968...   \n",
       "27546    [537899, 1810799, 1192753, 100524, 1817057, 56...   \n",
       "48869    [502278, 6512514, 14916864, 1874015, 64658630,...   \n",
       "68276    [1015554, 2765786, 1249788, 2379261, 105046, 8...   \n",
       "74646    [273861, 23929637, 18280, 273864, 6229228, 273...   \n",
       "...                                                    ...   \n",
       "2498568  [2426727, 21505358, 43597609, 15313686, 407875...   \n",
       "2502247  [64629537, 52890761, 56156380, 24270915, 59343...   \n",
       "2525292  [16920761, 47026229, 66214585, 19975563, 69519...   \n",
       "2525875  [68604288, 64782294, 86346988, 48600290, 17282...   \n",
       "2547242                     [15045560, 86179454, 18444525]   \n",
       "\n",
       "                                                  book_ids  works_count  \\\n",
       "22769    [31818, 376579, 130940, 36436103, 48672, 10047...          218   \n",
       "27546    [550655, 919995, 104260, 104262, 919987, 15247...          903   \n",
       "48869    [514313, 6326920, 10021420, 86161, 41433284, 9...           34   \n",
       "68276    [39988, 6310, 6319, 6689, 6327, 31456, 6693, 6...          825   \n",
       "74646    [282308, 17295645, 16602, 282311, 6053428, 282...          165   \n",
       "...                                                    ...          ...   \n",
       "2498568  [17900446, 15786351, 23997091, 10409565, 21469...           36   \n",
       "2502247  [41410287, 32262726, 34901031, 17418023, 37709...           22   \n",
       "2525292  [11958502, 26976387, 42502486, 14333723, 44849...           33   \n",
       "2525875  [44111406, 41509071, 30767954, 28462303, 12305...            8   \n",
       "2547242                     [10147283, 55265814, 13244210]            3   \n",
       "\n",
       "         author_id                   name  gender  \\\n",
       "22769         1203            Andy Warhol    male   \n",
       "27546         1464            George Sand  female   \n",
       "48869         2850            Ben Mezrich    male   \n",
       "68276         4273             Roald Dahl    male   \n",
       "74646         4743        Yoshitaka Amano    male   \n",
       "...            ...                    ...     ...   \n",
       "2498568   14381327   Dumitru Radu Popescu    male   \n",
       "2502247   14539438   Philippe Dautzenberg           \n",
       "2525292   15379169           Thomas Balch           \n",
       "2525875   15405966    Stanisław II August  female   \n",
       "2547242   16428797  Andrea        Ferrari    male   \n",
       "\n",
       "                                                 image_url  ...  \\\n",
       "22769    https://images.gr-assets.com/authors/120656521...  ...   \n",
       "27546    https://images.gr-assets.com/authors/134111243...  ...   \n",
       "48869    https://images.gr-assets.com/authors/140544550...  ...   \n",
       "68276    https://images.gr-assets.com/authors/131155490...  ...   \n",
       "74646    https://images.gr-assets.com/authors/124492114...  ...   \n",
       "...                                                    ...  ...   \n",
       "2498568  https://s.gr-assets.com/assets/nophoto/user/m_...  ...   \n",
       "2502247  https://s.gr-assets.com/assets/nophoto/user/u_...  ...   \n",
       "2525292  https://s.gr-assets.com/assets/nophoto/user/u_...  ...   \n",
       "2525875  https://images.gr-assets.com/authors/160040463...  ...   \n",
       "2547242  https://images.gr-assets.com/authors/148684958...  ...   \n",
       "\n",
       "        average_rating_y  text_reviews_count_y  language  \\\n",
       "22769               3.39                  22.0             \n",
       "27546               3.75                   0.0             \n",
       "48869               3.59                  39.0       eng   \n",
       "68276               4.25                   1.0             \n",
       "74646               5.00                   0.0       ger   \n",
       "...                  ...                   ...       ...   \n",
       "2498568             4.33                   0.0       rum   \n",
       "2502247             0.00                   0.0             \n",
       "2525292             0.00                   0.0             \n",
       "2525875             0.00                   0.0             \n",
       "2547242             3.43                   1.0       ita   \n",
       "\n",
       "                                 rating_dist ratings_count_y publication_date  \\\n",
       "22769     5:61|4:48|3:70|2:27|1:26|total:232           232.0       1998-02-17   \n",
       "27546            5:1|4:1|3:2|2:0|1:0|total:4             4.0                    \n",
       "48869    5:66|4:103|3:84|2:36|1:11|total:300           300.0       2015-01-01   \n",
       "68276            5:3|4:0|3:0|2:1|1:0|total:4             4.0       2005-05-30   \n",
       "74646            5:1|4:0|3:0|2:0|1:0|total:1             1.0                    \n",
       "...                                      ...             ...              ...   \n",
       "2498568         5:6|4:4|3:2|2:0|1:0|total:12            12.0             2000   \n",
       "2502247          5:0|4:0|3:0|2:0|1:0|total:0             0.0                    \n",
       "2525292          5:0|4:0|3:0|2:0|1:0|total:0             0.0                    \n",
       "2525875          5:0|4:0|3:0|2:0|1:0|total:0             0.0                    \n",
       "2547242          5:1|4:2|3:3|2:1|1:0|total:7             7.0       2008-03-28   \n",
       "\n",
       "        edition_information           publisher num_pages title_length  \n",
       "22769                               Grove Press       384          1.0  \n",
       "27546                                                              1.0  \n",
       "48869                                  FG Press       160          1.0  \n",
       "68276                                         강       353          1.0  \n",
       "74646                                                              1.0  \n",
       "...                     ...                 ...       ...          ...  \n",
       "2498568                   1              Gramar       316          1.0  \n",
       "2502247                                                            1.0  \n",
       "2525292                                                            1.0  \n",
       "2525875                                                            1.0  \n",
       "2547242      Le vele series  Fazi Editore, Roma                    1.0  \n",
       "\n",
       "[236 rows x 31 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_authors_books[df_authors_books[\"title_length\"]==1]\n",
    "# there are 236 titles with length equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(920, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count_x</th>\n",
       "      <th>average_rating_x</th>\n",
       "      <th>text_reviews_count_x</th>\n",
       "      <th>work_ids</th>\n",
       "      <th>book_ids</th>\n",
       "      <th>works_count</th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>image_url</th>\n",
       "      <th>...</th>\n",
       "      <th>average_rating_y</th>\n",
       "      <th>text_reviews_count_y</th>\n",
       "      <th>language</th>\n",
       "      <th>rating_dist</th>\n",
       "      <th>ratings_count_y</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>publisher</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7563</th>\n",
       "      <td>2481167</td>\n",
       "      <td>4.12</td>\n",
       "      <td>62472</td>\n",
       "      <td>[153313, 1477756, 866393, 6151926, 2999000, 23...</td>\n",
       "      <td>[5471, 2794, 415, 5933841, 5809, 17208457, 413...</td>\n",
       "      <td>46</td>\n",
       "      <td>235</td>\n",
       "      <td>Thomas Pynchon</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/146536115...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1059.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>5:6852|4:7307|3:3737|2:1100|1:536|total:19532</td>\n",
       "      <td>19532.0</td>\n",
       "      <td>2005-07-05</td>\n",
       "      <td></td>\n",
       "      <td>Harper Perennial Modern Classics</td>\n",
       "      <td>547</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9255</th>\n",
       "      <td>205867</td>\n",
       "      <td>3.75</td>\n",
       "      <td>13441</td>\n",
       "      <td>[929649, 2226163, 6518120, 6574787, 46813511, ...</td>\n",
       "      <td>[599, 597, 24475, 6386555, 27068734, 24476, 21...</td>\n",
       "      <td>117</td>\n",
       "      <td>375</td>\n",
       "      <td>Chuck Klosterman</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/133606020...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>5:0|4:1|3:0|2:1|1:0|total:2</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22769</th>\n",
       "      <td>47012</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1135</td>\n",
       "      <td>[1283844, 366428, 382424, 58136682, 47615, 968...</td>\n",
       "      <td>[31818, 376579, 130940, 36436103, 48672, 10047...</td>\n",
       "      <td>218</td>\n",
       "      <td>1203</td>\n",
       "      <td>Andy Warhol</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/120656521...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.39</td>\n",
       "      <td>22.0</td>\n",
       "      <td></td>\n",
       "      <td>5:61|4:48|3:70|2:27|1:26|total:232</td>\n",
       "      <td>232.0</td>\n",
       "      <td>1998-02-17</td>\n",
       "      <td></td>\n",
       "      <td>Grove Press</td>\n",
       "      <td>384</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ratings_count_x  average_rating_x  text_reviews_count_x  \\\n",
       "7563           2481167              4.12                 62472   \n",
       "9255            205867              3.75                 13441   \n",
       "22769            47012              3.81                  1135   \n",
       "\n",
       "                                                work_ids  \\\n",
       "7563   [153313, 1477756, 866393, 6151926, 2999000, 23...   \n",
       "9255   [929649, 2226163, 6518120, 6574787, 46813511, ...   \n",
       "22769  [1283844, 366428, 382424, 58136682, 47615, 968...   \n",
       "\n",
       "                                                book_ids  works_count  \\\n",
       "7563   [5471, 2794, 415, 5933841, 5809, 17208457, 413...           46   \n",
       "9255   [599, 597, 24475, 6386555, 27068734, 24476, 21...          117   \n",
       "22769  [31818, 376579, 130940, 36436103, 48672, 10047...          218   \n",
       "\n",
       "       author_id              name gender  \\\n",
       "7563         235    Thomas Pynchon   male   \n",
       "9255         375  Chuck Klosterman   male   \n",
       "22769       1203       Andy Warhol   male   \n",
       "\n",
       "                                               image_url  ...  \\\n",
       "7563   https://images.gr-assets.com/authors/146536115...  ...   \n",
       "9255   https://images.gr-assets.com/authors/133606020...  ...   \n",
       "22769  https://images.gr-assets.com/authors/120656521...  ...   \n",
       "\n",
       "      average_rating_y  text_reviews_count_y  language  \\\n",
       "7563              3.96                1059.0       eng   \n",
       "9255              3.00                   0.0             \n",
       "22769             3.39                  22.0             \n",
       "\n",
       "                                         rating_dist ratings_count_y  \\\n",
       "7563   5:6852|4:7307|3:3737|2:1100|1:536|total:19532         19532.0   \n",
       "9255                     5:0|4:1|3:0|2:1|1:0|total:2             2.0   \n",
       "22769             5:61|4:48|3:70|2:27|1:26|total:232           232.0   \n",
       "\n",
       "      publication_date edition_information                         publisher  \\\n",
       "7563        2005-07-05                      Harper Perennial Modern Classics   \n",
       "9255                                                                           \n",
       "22769       1998-02-17                                           Grove Press   \n",
       "\n",
       "      num_pages title_length  \n",
       "7563        547          2.0  \n",
       "9255                     2.0  \n",
       "22769       384          1.0  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 920 rows without a name\n",
    "print(df_authors_books[df_authors_books[\"title_length\"]<3].shape)\n",
    "df_authors_books[df_authors_books[\"title_length\"]<3].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ5] We can consider the authors with the most fans to be influential. Let’s have a deeper look.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the top 10 most influential authors regarding their fan count and number of books. Who is the most influential author?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the useful columns of the 2 datasets to work properly on it and to store less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_b_2 = books_cleaned[[\"id\", \"title\", \"author_id\", \"author_name\", \"series_name\", \"series_id\", \"series_position\", \"format\", \"original_publication_date\", \"work_id\", \"average_rating\", \"text_reviews_count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the 10 most influential authors regarding their fan_count. We use the dataset \"authors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_fc = authors.sort_values(by=authors.columns[11], ascending = False)\n",
    "\n",
    "fc_columns = df_sorted_fc.iloc[:, [6, 7, 11]]\n",
    "\n",
    "\n",
    "most_influent_authors = dict()\n",
    "count = 0\n",
    "\n",
    "for i, j in zip(fc_columns[\"name\"], fc_columns[\"id\"]):\n",
    "    most_influent_authors[i] = j\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break\n",
    "\n",
    "most_influent_authors\n",
    "fc_columns.head(10) # to print only the first 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work on cleaning up the dataset with respect to the publication date, as it contains various date formats that need to be standardized for a better analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on the data_frame to fix the variable publication_date and create a new column formatted as \"%Y-%m-%d\"\n",
    "\n",
    "opd_column = df_5_b_2[\"original_publication_date\"]\n",
    "opd_column_formatted = []\n",
    "\n",
    "def\n",
    "for i in opd_column:\n",
    "    if len(i) in [4, 7, 10]:\n",
    "        if len(i) == 4:\n",
    "            i += \"-01-01\"\n",
    "\n",
    "        if len(i) == 7:\n",
    "            i += \"-01\"\n",
    "        try:\n",
    "            format = \"%Y-%m-%d\"\n",
    "            timestamp_formattato = datetime.datetime.strptime(i, format)\n",
    "            opd_column_formatted.append(timestamp_formattato)\n",
    "        except ValueError as ve:\n",
    "            opd_column_formatted.append(pd.NaT)\n",
    "    else:\n",
    "        opd_column_formatted.append(pd.NaT)\n",
    "\n",
    "df_5_b_2['opd_column_formatted'] = opd_column_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also clean 'original_publication_date' column deleting outliers.\n",
    "\n",
    "We have chosen to remove outliers using the 1st percentile as the lower limit and today's date as the upper limit. However, there are different ways to implement this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_b_2['opd_column_formatted'] = pd.to_datetime(df_5_b_2['opd_column_formatted'], errors='coerce')\n",
    "\n",
    "Q_01 = df_5_b_2['opd_column_formatted'].quantile(0.01)\n",
    "\n",
    "# Calculate the limits for outliers\n",
    "lower_limit = pd.to_datetime(Q_01)\n",
    "upper_limit = pd.to_datetime(\"2023-11-02 00:00:00\")\n",
    "\n",
    "# Filter the DataFrame to remove outliers\n",
    "df_5_b_2 = df_5_b_2[(df_5_b_2['opd_column_formatted'] >= lower_limit) & (df_5_b_2['opd_column_formatted'] <= upper_limit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique works_id for every author_id\n",
    "count_work_id = df_5_b_2.groupby('author_id')['work_id'].nunique()\n",
    "\n",
    "# Get the result as dataframe\n",
    "result_df = count_work_id.reset_index()\n",
    "\n",
    "# Rename columns\n",
    "result_df.columns = ['author_id', 'num_works']\n",
    "\n",
    "result_df = result_df.merge(df_5_b_2[['author_id', 'author_name']].drop_duplicates(), on='author_id', how='left')\n",
    "\n",
    "result_df_books = result_df.sort_values(by='num_works', ascending=False)\n",
    "\n",
    "result_df_books.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table above represents the first 10 \"authors\", sorted by their publications. As expected they are pretty much all companies or classes such as 'Various' or 'UNKNOWN'. \n",
    "\n",
    "But we can visualize the table without these elements with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['NOT A BOOK', 'Unknown', 'Various', 'Anonymous']\n",
    "result_df_books = result_df_books.loc[~result_df_books['author_name'].isin(x)]\n",
    "result_df_books.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_5 = []\n",
    "result_df = result_df[result_df['author_id'].isin(most_influent_authors.values())]\n",
    "result_df\n",
    "\n",
    "for key, value in most_influent_authors.items():\n",
    "    result_df_5.append([key, value, result_df[result_df['author_id'] == value]['num_works'].iloc[0], authors[authors['id'] == value]['fans_count'].iloc[0]])\n",
    "\n",
    "result_df_5 = pd.DataFrame(result_df_5)\n",
    "result_df_5.columns = ['author_name', 'author_id', 'number_of_books', 'fans_count']\n",
    "result_df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most influent author is Stephen King with 776035 fans and 375 books published. \n",
    "\n",
    "It's not surprising that he has so many fans, considering he's the author with the most books published among the top 10 influential authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Have they published any series of books? If any, extract the longest series name among these authors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_series = []\n",
    "\n",
    "for i in most_influent_authors:\n",
    "    selected_row = df_5_b_2.loc[df_5_b_2['author_id'] == most_influent_authors[i]]\n",
    "    variable_value = selected_row['series_name'].values\n",
    "    variable_value = list(set(variable_value))\n",
    "    variable_value.remove('')\n",
    "        \n",
    "    if variable_value == []:\n",
    "        print(f\"The author {i} hasn't written any series.\")\n",
    "        print()\n",
    "    else:\n",
    "        count = 1\n",
    "        print(f\"The author {i} has written the next series: \")\n",
    "        for j in variable_value:\n",
    "            print(f\"{count}. {j}\")\n",
    "            count += 1\n",
    "            list_series.append(j)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bill Gates and Mindy Kaling have not authored any series. The other authors have written numerous series, many of which have been translated into multiple languages and published under different titles in various locations. For instance, Stephen King, an American author, is highly popular in Japan, and as a result, some of his series are also available there, as indicated by the previous code's output.\n",
    "\n",
    "Now, let's extract the longest series name among those found earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_string = \"\"\n",
    "\n",
    "for string in list_series:\n",
    "    if len(string) > len(longest_string):\n",
    "        longest_string = string\n",
    "\n",
    "sorted_list = sorted(list_series, key=len)\n",
    "print(longest_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SERIES_POSITION CHECKKK #########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest series name is: Percy Jackson and the Olympians: The Graphic Novels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many of these authors have been published in different formats? Provide a meaningful chart on the distribution of the formats and comment on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting let's create a new database without the books that don't have a format starting from the uncleaned database \"books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_format = books[books['format'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To answer this question we use the original database and not \"df_5_b_2\" because we need to evaluate all kinds of formats published by an author,\n",
    "# even if there are more formats for the same opera.\n",
    "\n",
    "for i in most_influent_authors:\n",
    "    sel_row = books_format.loc[books_format['author_id'] == most_influent_authors[i]]\n",
    "    value_var = sel_row['format'].values\n",
    "    app = Counter(value_var)\n",
    "    count = 0\n",
    "    new_dict = {}\n",
    "    \n",
    "    total = sum(app.values())\n",
    "    \n",
    "    for key in app:\n",
    "        if app[key]/total < 0.02:\n",
    "            count += np.round(app[key]/total, 3)\n",
    "        else:\n",
    "            new_dict[key] = np.round(app[key]/total, 3)\n",
    "    \n",
    "    if count != 0:\n",
    "        new_dict['Others'] = count\n",
    "\n",
    "    # print(f\"{i} ha: {new_dict}\")\n",
    "\n",
    "    cat = new_dict.keys()  \n",
    "    val = new_dict.values()   \n",
    "    plt.bar(cat, val)\n",
    "\n",
    "    # add labels\n",
    "    plt.xlabel(\"Format\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.title(f\"{i}'s format works histogram\")\n",
    "    plt.xticks(rotation = 90)\n",
    "    \n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs display the distributions of book formats for each of the most influential authors. To streamline the presentation, we have categorized all formats that represent less than 2% into a class labeled 'Others' to prevent an excessive number of different formats.\n",
    "\n",
    "\n",
    "We can observe that the most prevalent formats are Paperback and Hardcover, followed by eBooks and Audiobooks. This highlights how paper has not yet been completely overtaken by digital formats and how people still appreciate purchasing books as they used to.\n",
    "\n",
    "We can also notice that more recent authors tend to use digital publications, unlike older authors. Furthermore authors that has more series use more different formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Provide information about the general response from readers (number of fans, average rating, number of reviews, etc.), divide the authors by gender, and comment about anything eventually related to “structural bias”. You can even ask ChatGPT or any other LLM chatbot tool: try to formulate a prompt that provides helpful information about it. Put that information in your notebook and provide comments on what you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors_10 = authors[authors['id'].isin(most_influent_authors.values())]\n",
    "\n",
    "df_authors_10 = df_authors_10[df_authors_10['gender'] != \"\"]\n",
    "\n",
    "## df_authors_10 = authors[authors['gender'].isin(['male', 'female'])]\n",
    "\n",
    "grouped = df_authors_10.groupby('gender')\n",
    "\n",
    "# Calculate mean, median, and standard deviation for each group\n",
    "summary_stats = grouped.agg({\n",
    "    'fans_count': ['mean', 'median', 'std'],\n",
    "    'average_rating': ['mean', 'median', 'std'],\n",
    "    'text_reviews_count': ['mean', 'median', 'std']\n",
    "})\n",
    "\n",
    "# Rename the columns for clarity\n",
    "summary_stats.columns = ['fans_mean', 'fans_median', 'fans_sd',\n",
    "                         'rating_mean', 'rating_median', 'rating_sd',\n",
    "                         'reviews_mean', 'reviews_median', 'reviews_sd']\n",
    "\n",
    "# Reset the index to make 'gender' a regular column\n",
    "summary_stats.reset_index(inplace=True)\n",
    "\n",
    "# Display the summary statistics\n",
    "summary_stats\n",
    "# df_authors_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ6] For this question, consider the top 10 authors concerning the number of fans again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Provide the average time gap between two subsequent publications for a series of books and those not belonging to a series. What do you expect to see, and what is the actual answer to this question?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, we need to clean the 'series_position' column first. This is because some series have a sort of spin-off that contains more than one volume, and there are also some kinds of books that are not part of the main saga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(most_influent_authors.values())\n",
    "\n",
    "# Filter the original dataframe based on the ID's \n",
    "df_initial = df_5_b_2[df_5_b_2['author_id'].isin(ids)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only considering books that are part of a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = df_initial.dropna(subset=['opd_column_formatted', 'series_id', 'series_position']) # dropping all the books that are not part of a series\n",
    "df_series = df_series[df_series['series_id'] != \"\"]\n",
    "df_series = df_series[df_series['series_position'] != \"\"]\n",
    "len(df_series) # 368 books not belonging to a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = df_series.sort_values(by=['series_id', 'series_position'])\n",
    "\n",
    "df_series['time_gap'] = df_series.groupby('series_id')['opd_column_formatted'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series['time_gap'] = df_series['time_gap'].astype(str) \n",
    "df_series['time_gap'] = df_series['time_gap'].replace('0:00:00', np.nan) # we are not considering time gaps equal to 0\n",
    "df_series['time_gap'] = pd.to_timedelta(df_series['time_gap'])\n",
    "\n",
    "media_timegap_series = df_series['time_gap'].mean()\n",
    "print(f\"Time gap between 2 consecutive publications of books belonging to a series is: {media_timegap_series.days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out the average time gap between two subsequent publications for a series of books is equal to **340 days**, equal to 1 year more or less.\n",
    "\n",
    "Before delving into the solution of the other average time between two books not belonging to a series, it's important to an assumption.\n",
    "\n",
    "We expect that there will be a shorter time gap between the consecutive publications of books within a series. This expectation is based on the likelihood that an author often works on multiple books within the same series simultaneously or begins working on the next one almost immediately after releasing the most recent installment. This is in contrast to two books that are not part of the same series, where we might expect longer intervals between their publications.\n",
    "\n",
    "So let's create a new subset to analyze the time gap between two books not belonging to a series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_series = df_initial.drop(columns = ['series_position', 'original_publication_date']) # we work again on the original dataset because we need also the NA's series_id\n",
    "df_not_series = df_not_series[df_not_series['series_id'] == \"\"]\n",
    "df_not_series = df_not_series.sort_values(by=['author_id', 'opd_column_formatted'])\n",
    "\n",
    "print(len(df_not_series)) # 447 books within a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_gap_books = df_not_series.groupby('author_id')['opd_column_formatted'].diff()\n",
    "df_not_series['time_gap_books'] = time_gap_books\n",
    "df_not_series['time_gap_books'] = df_not_series['time_gap_books'].replace(datetime.timedelta(0), pd.NaT) # we are not considering timedelta(0)\n",
    "df_not_series['time_gap_books'] = df_not_series['time_gap_books'].replace(np.nan, pd.NaT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's finally compute the mean\n",
    "\n",
    "a = datetime.timedelta(0)\n",
    "conta = 0\n",
    "\n",
    "for i in df_not_series['time_gap_books']:\n",
    "    if str(type(i)) != \"<class 'pandas._libs.tslibs.nattype.NaTType'>\":\n",
    "        a += i\n",
    "        conta += 1\n",
    "\n",
    "print(f\"Time gap between 2 consecutive publications of books not belonging to a series is {(a/conta).days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unexpected, the time gap between the release of two consecutive books, which are not part of a series, is **204 days**. Notably, this 204-day gap is approximately two-thirds of the time gap calculated for some other context.\n",
    "\n",
    "############# COMMENTSSS #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each of the authors, give a convenient plot showing how many books has the given author published UP TO a given year. Are these authors contemporary with each other? Can you notice a range of years where their production rate was higher?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by creating a new column of the subset with the year of the original publication of every book\n",
    "\n",
    "df_6 = df_5_b_2[[\"author_id\", \"original_publication_date\", \"work_id\"]]\n",
    "\n",
    "df_6 = df_6[df_6['author_id'].isin(ids)]\n",
    "\n",
    "opd_column = df_6[\"original_publication_date\"]\n",
    "year_of_publication = []\n",
    "\n",
    "for i in opd_column:\n",
    "    if len(i) in [4, 7, 10]:\n",
    "        if len(i) == 4:\n",
    "            i += \"-01-01\"\n",
    "\n",
    "        if len(i) == 7:\n",
    "            i += \"-01\"\n",
    "        \n",
    "        format = \"%Y-%m-%d\"\n",
    "        timestamp_format = datetime.datetime.strptime(i, format)\n",
    "        year_of_publication.append(timestamp_format.year)\n",
    "    else:\n",
    "        year_of_publication.append(pd.NaT)\n",
    "\n",
    "df_6['opd_column_formatted'] = year_of_publication\n",
    "df_6 = df_6.dropna(subset=['opd_column_formatted'])\n",
    "df_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######## COMMENTS #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in most_influent_authors:\n",
    "    row = df_6.loc[df_6['author_id'] == most_influent_authors[i]]\n",
    "    vv = row['opd_column_formatted'].values\n",
    "    app = Counter(vv)\n",
    "    count = 0\n",
    "    \n",
    "    # print(f\"{i} ha: {app}\")\n",
    "\n",
    "    cat = sorted(app.keys())\n",
    "    val = app.values()\n",
    "    cumulative_hist = []\n",
    "    i_esimo = 0\n",
    "    \n",
    "    for year in cat:\n",
    "        i_esimo += app[year]\n",
    "        cumulative_hist.append(i_esimo)\n",
    "            \n",
    "        \n",
    "    # print(cat, cumulative_hist)\n",
    "    # plt.bar(cat, cumulative_hist)\n",
    "    \n",
    "    for j in range(len(cumulative_hist) - 1):\n",
    "        plt.plot([cat[j], cat[j + 1]], [cumulative_hist[j], cumulative_hist[j]], color='b', linestyle='-')\n",
    "\n",
    "    plt.hlines(cumulative_hist[-1], cat[-1], cat[-1] + 0.5, colors='b', linestyles='-')\n",
    "\n",
    "    # add labels\n",
    "    plt.xlabel(\"Years\")\n",
    "    plt.ylabel(\"Books published\")\n",
    "    plt.title(f\"{i}'s cumulative distribution of works\")\n",
    "    plt.xticks(rotation = 90)\n",
    "    for j in range(1, len(cumulative_hist)):\n",
    "        plt.vlines(cat[j], cumulative_hist[j-1], cumulative_hist[j], colors='b', linestyles='-')\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen to represent the cumulative distribution of published books using the ECDF (Empirical Cumulative Distribution Function) without normalization. This approach allows us to visualize the publication distribution up to a specific year more effectively.\n",
    "\n",
    "\n",
    "It seems that all these authors are roughly contemporary with each other. Some, like Stephen King, have had long careers spanning from 1960 to the present day, while others, such as Bill Gates, have only published a few books. Their book production appears to be more consistent during the first two decades of the 2000s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS POINTS\n",
    "***1.***\n",
    "Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload authors.json dataset, and filter authors with at least 100 reviews. \n",
    "\n",
    "Do the same using Pandas and compare performance in terms of milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "file_path = \"./lighter_authors.json\"\n",
    "\n",
    "# Read the JSON file into a Dask DataFrame\n",
    "df_9 = dd.read_json(file_path)\n",
    "\n",
    "# Filter authors with at least 100 reviews\n",
    "result = df_9[df_9['text_reviews_count'] >= 100]\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the result and the time taken in milliseconds\n",
    "result\n",
    "print(f\"Time taken using Dask: {(end_time - start_time) * 1000} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "file_path = \"./lighter_authors.json\"\n",
    "\n",
    "# Read the JSON file into a Dask DataFrame\n",
    "df_9 = pd.read_json(file_path, lines = True)\n",
    "\n",
    "# Filter authors with at least 100 reviews\n",
    "result = df_9[df_9['text_reviews_count'] >= 100]\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the result and the time taken in milliseconds\n",
    "result\n",
    "print(f\"Time taken using Pandas: {(end_time - start_time) * 1000} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading file \"lighter_authors\" and filtering authors with more than 100 reviews lasts about one half using Pandas instead of Dask, **32262 ms** against **13582 ms**.\n",
    "So for this kind of operations is more convenient using the first library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2*** Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload books.json, and join them with authors.json based on author_id. How many books don’t have a match for the author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DO NOT RUN ####\n",
    "file_path_1 = \"./lighter_authors.json\"\n",
    "file_path_2 = \"./lighter_books.json\"\n",
    "\n",
    "# Read the \"books.json\" and \"authors.json\" datasets into Dask DataFrames\n",
    "books_df = dd.read_json(file_path_2)\n",
    "authors_df = dd.read_json(file_path_1)\n",
    "\n",
    "# Perform the inner join on 'author_id'\n",
    "joined_df = dd.merge(books_df, authors_df, on='author_id', how='inner')\n",
    "\n",
    "# Find the number of books without a matching author\n",
    "books_without_author = len(books_df) - len(joined_df)\n",
    "print(f\"Number of books without a matching author: {books_without_author.compute()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_authors = \".\\lighter_authors.json\"\n",
    "ch_size = 100\n",
    "dfs = []\n",
    "\n",
    "# to import only some columns\n",
    "with open(light_authors, \"r\") as file:\n",
    "        for ch in pd.read_json(file, lines = True, chunksize = ch_size):\n",
    "                ch = ch[[\"name\", \"works_count\",\"ratings_count\",\"average_rating\",\n",
    "\"text_reviews_count\", \"work_ids\", \"book_ids\",\"id\",\n",
    "\"gender\",\n",
    "\"about\",\"fans_count\"]]\n",
    "        dfs.append(ch)\n",
    "\n",
    "la = pd.concat(dfs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ7]  Estimating probabilities is a core skill for a data scientist: show us your best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estimate the probability that a book has over 30% of the ratings above 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lighter_books = \".\\lighter_books.json\"\n",
    "ch_size = 100\n",
    "dfbs = []\n",
    "\n",
    "# we import the dataset\n",
    "with open(lighter_books, \"r\") as fileb:\n",
    "    rows_read = 0\n",
    "    for chb in pd.read_json(fileb, lines = True, chunksize = ch_size):\n",
    "        chb = chb[[\"id\", \"title\", \"author_name\",\"author_id\", \"work_id\",\n",
    "\"language\",\"average_rating\",\"rating_dist\",\"ratings_count\",\"text_reviews_count\",\"publication_date\",\n",
    "\"original_publication_date\", \"format\",\"edition_information\",\"publisher\",\"num_pages\",\n",
    "\"series_id\",\"series_name\",\"series_position\"]]\n",
    "        dfbs.append(chb)\n",
    "#       Update the count of rows read\n",
    "        rows_read += len(chb)\n",
    "        \n",
    "        # Check if we have read 10,000 rows, and if so, break the loop\n",
    "        if rows_read >= 100000:\n",
    "            break\n",
    "            \n",
    "#final data with our books\n",
    "lb = pd.concat(dfbs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we chose the data we want to study, in this case lighter_books\n",
    "dr = books[['rating_dist','ratings_count']]\n",
    "\n",
    "#we change rating_dist to str to be able to separate the values of each rating\n",
    "dr['rating_dist'] = dr['rating_dist'].astype(str)\n",
    "\n",
    "\n",
    "#we define a function to separate the ratings we want\n",
    "def parse_rating_dist(rating_dist):\n",
    "    match = re.search(r'5:(\\d+)', rating_dist)\n",
    "    if match:\n",
    "        ratings_5 = int(match.group(1))\n",
    "    else:\n",
    "        ratings_5 = 0\n",
    "\n",
    "    match = re.search(r'4:(\\d+)', rating_dist)\n",
    "    if match:\n",
    "        ratings_4 = int(match.group(1))\n",
    "    else:\n",
    "        ratings_4 = 0  \n",
    "\n",
    "    return ratings_4, ratings_5\n",
    "\n",
    "# Apply the parsing function\n",
    "dr['ratings_4'], dr['ratings_5'] = zip(*dr['rating_dist'].map(parse_rating_dist))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the probability\n",
    "dr['proportion_above_4'] = (dr['ratings_4'] + dr['ratings_5']) / dr['ratings_count']\n",
    "\n",
    "\n",
    "# Estimate the probability\n",
    "probability_above_30_percent = (dr['proportion_above_4'] > 0.30).mean()\n",
    "\n",
    "print(probability_above_30_percent)\n",
    "\n",
    "print(f\"Estimated probability that a book has over 30% of ratings above 4: {probability_above_30_percent:.2%}\")\n",
    "pr = 0\n",
    "for i in range(0,len(dr)):\n",
    "    if dr['proportion_above_4'] [i] > 0.30:\n",
    "        pr += 1\n",
    "print('The probability to have over 30% of above 4 is',pr/(len(dr))*100,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estimate the probability that an author publishes a new book within two years from its last work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we get the columsn we'll need\n",
    "dt = lb[['title','author_id','publication_date','original_publication_date']]\n",
    "\n",
    "# we change the  datetime to be able touse it more easily\n",
    "dt['original_publication_date'] = pd.to_datetime(dt['original_publication_date'], errors='coerce')\n",
    "\n",
    "dt.sort_values(by=['author_id', 'original_publication_date'], ascending=True, inplace=True)\n",
    "print(dt)\n",
    "# Calculate time gaps\n",
    "dt['time_gap'] = dt.groupby('author_id')['original_publication_date'].diff()\n",
    "print(dt)\n",
    "\n",
    "# Handle invalid (out-of-bounds) dates by replacing them with the previous work's publication date\n",
    "dt['original_publication_date'] = dt.groupby('author_id')['original_publication_date'].fillna(method='ffill')\n",
    "print(dt)\n",
    "\n",
    "# Filter the data\n",
    "filtered_data = dt.dropna(subset=['original_publication_date', 'time_gap'])\n",
    "print(filtered_data)\n",
    "filtered_data = filtered_data[filtered_data.groupby('author_id')['author_id'].transform('size') > 1]\n",
    "print(filtered_data)\n",
    "# Calculate the probability\n",
    "probability_within_two_years = dt.groupby('author_id')['time_gap'].apply(lambda x: (x <= timedelta(days=730)).any()).mean()\n",
    "\n",
    "print(f\"Estimated probability that an author publishes a new book within two years from their last work: {probability_within_two_years:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the file list.json, you will find a peculiar list named \"The Worst Books of All Time.\" Estimate the probability of a book being included in this list, knowing it has more than 700 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"list.json\"\n",
    "ch_size = 100\n",
    "dfls = []\n",
    "\n",
    "# to import only some columns\n",
    "with open(file_pathl, \"r\") as filel:\n",
    "    rows_read = 0\n",
    "    for chl in pd.read_json(filel, lines = True, chunksize = ch_size):\n",
    "        chl = chl[[\"id\",\n",
    "\"title\",\n",
    "\"description\",\"description_html\",\"num_pages\",\n",
    "\"num_books\",\n",
    "\"num_voters\",\n",
    "\"created_date\",\n",
    "\"tags\",\n",
    "\"num_likes\",\n",
    "\"created_by\",\n",
    "\"num_comments\",\n",
    "\"books\"]]\n",
    "        dfls.append(chl)\n",
    "#          Update the count of rows read\n",
    "        rows_read += len(chl)\n",
    "        \n",
    "        # Check if we have read 10,000 rows, and if so, break the loop\n",
    "        if rows_read >= 100000:\n",
    "            break\n",
    "\n",
    "dfl = pd.concat(dfls, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lb['num_pages'] = pd.to_numeric(lb['num_pages'], errors='coerce')  # Convert to integers and handle errors\n",
    "books_with_more_than_700_pages = lb[lb['num_pages'] > 700]\n",
    "\n",
    "# Step 2: Identify books in \"The Worst Books of All Time\" list\n",
    "# Assuming 'list' contains information about the list, extract the relevant book titles from it\n",
    "rs = dfl[dfl['title'] == \"The Worst Books of All Time\"]\n",
    "\n",
    "# Extract the titles of the books in the list\n",
    "\n",
    "# Create a new Series containing book dictionaries\n",
    "book_series = rs['books'].apply(lambda x: [item['title'] for item in x])\n",
    "\n",
    "# Create a flattened list of titles\n",
    "book_titles = [title for titles in book_series for title in titles]\n",
    "\n",
    "# Step 2: Identify books in the extracted list\n",
    "books_in_extracted_list = books_with_more_than_700_pages[books_with_more_than_700_pages['title'].isin(book_titles)]\n",
    "\n",
    "# Step 3: Calculate the probability\n",
    "total_books = len(books_with_more_than_700_pages)\n",
    "books_in_list_count = len(books_in_extracted_list)\n",
    "\n",
    "probability_in_list_given_700_pages = books_in_list_count / total_books\n",
    "\n",
    "print(probability_in_list_given_700_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'lb' is a DataFrame containing book information\n",
    "# 'list' is a DataFrame containing list information\n",
    "\n",
    "# Step 1: Filter books with more than 700 pages from 'lb'\n",
    "# Convert 'num_pages' to integers and then filter\n",
    "lb['num_pages'] = pd.to_numeric(lb['num_pages'], errors='coerce')  # Convert to integers and handle errors\n",
    "books_with_more_than_700_pages = lb[lb['num_pages'] > 700]\n",
    "\n",
    "# Step 2: Identify books in \"The Worst Books of All Time\" list\n",
    "# Assuming 'list' contains information about the list, extract the relevant book titles from it\n",
    "worst_books_list = dfl[dfl['title'] == \"The Worst Books of All Time\"]\n",
    "\n",
    "# Step 3: Calculate the probability\n",
    "total_books = len(books_with_more_than_700_pages)\n",
    "books_in_worst_list_count = len(worst_books_list)\n",
    "\n",
    "probability_on_worst_list = books_in_worst_list_count / total_books\n",
    "\n",
    "print(f\"Estimated probability of a book being on 'The Worst Books of All Time' list, given it has more than 700 pages: {probability_on_worst_list:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate P(X), the probability of being in the list\n",
    "total_books = len(books_with_more_than_700_pages)  # Number of books with more than 700 pages\n",
    "books_in_worst_list_count = len(books_in_extracted_list)  # Number of books in the list\n",
    "\n",
    "P_X = books_in_worst_list_count / total_books\n",
    "\n",
    "# Calculate P(X|Y), the probability of being in the list given more than 700 pages\n",
    "total_books_with_more_than_700_pages = len(books_with_more_than_700_pages)  # Total books with more than 700 pages\n",
    "books_in_extracted_list_count = len(books_in_extracted_list)  # Number of books in the list with more than 700 pages\n",
    "P_X_given_Y = books_in_extracted_list_count / total_books_with_more_than_700_pages\n",
    "\n",
    "# Compare the probabilities\n",
    "if P_X_given_Y == P_X:\n",
    "    print(\"The events X and Y are independent.\")\n",
    "else:\n",
    "    print(\"The events X and Y are dependent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pathb = \"lighter_books.json\"\n",
    "ch_size = 100\n",
    "dfbs = []\n",
    "\n",
    "# to import only some columns\n",
    "with open(file_pathb, \"r\") as fileb:\n",
    "    rows_read = 0\n",
    "    for chb in pd.read_json(fileb, lines = True, chunksize = ch_size):\n",
    "        chb = chb[[\"id\", \"title\", \"author_name\",\"author_id\", \"work_id\",\n",
    "\"language\",\"average_rating\",\"rating_dist\",\"ratings_count\",\"text_reviews_count\",\"publication_date\",\n",
    "\"original_publication_date\", \"format\",\"edition_information\",\"publisher\",\"num_pages\",\n",
    "\"series_id\",\"series_name\",\"series_position\"]]\n",
    "        dfbs.append(chb)\n",
    "         # Update the count of rows read\n",
    "        rows_read += len(chb)\n",
    "        \n",
    "        # Check if we have read 10,000 rows, and if so, break the loop\n",
    "        if rows_read >= 100000:\n",
    "            break\n",
    "\n",
    "lb = pd.concat(dfbs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
    "dav = lb[['average_rating','num_pages']]\n",
    "\n",
    "# Data preparation and cleaning\n",
    "dav['average_rating'] = dav['average_rating'].astype(float)\n",
    "\n",
    "# Remove rows with missing or empty 'num_pages' values\n",
    "dav = dav[dav['num_pages'].str.strip() != '']\n",
    "dav['num_pages'] = dav['num_pages'].astype(float)\n",
    "\n",
    "# Define the book length groups\n",
    "# You can customize the cutoffs for short, medium, and long books based on your data.\n",
    "short = dav[dav['num_pages'] <= 150]\n",
    "med = dav[(dav['num_pages'] > 150) & (dav['num_pages'] <= 700)]\n",
    "long = dav[dav['num_pages'] > 700]\n",
    "\n",
    "# Calculate summary statistics for average ratings\n",
    "mean_short = short['average_rating'].mean()\n",
    "mean_med = med['average_rating'].mean()\n",
    "mean_long = long['average_rating'].mean()\n",
    "\n",
    "# Perform hypothesis testing (e.g., ANOVA) to determine if there are significant differences\n",
    "f_statistic, p_value = stats.f_oneway(short['average_rating'], med['average_rating'], long['average_rating'])\n",
    "\n",
    "print(f\"Mean rating for short books: {mean_short:.2f}\")\n",
    "print(f\"Mean rating for medium books: {mean_med:.2f}\")\n",
    "print(f\"Mean rating for long books: {mean_long:.2f}\")\n",
    "\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"There are statistically significant differences in ratings between book length groups.\")\n",
    "else:\n",
    "    print(\"There are no statistically significant differences in ratings between book length groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot demonstrate that usually readers rate the longest books as the worst, since as we just see is normally thte contrary, the longest the book the better ratings it gets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the average rate distribution for English and non-English books with a proper statistical procedure. What can you conclude about those two groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
    "deng = lb[['average_rating','language']]\n",
    "\n",
    "# Data preparation\n",
    "deng['average_rating'] = deng['average_rating'].astype(float)\n",
    "deng['language'] = deng['language'].str.lower()  # Ensure uniform case for language comparison\n",
    "\n",
    "# Split data into English and non-English books\n",
    "eng = deng[deng['language'] == 'eng']\n",
    "neng = deng[deng['language'] != 'eng']\n",
    "\n",
    "meng=eng['average_rating'].mean()\n",
    "mneng=neng['average_rating'].mean()\n",
    "\n",
    "print(f\"Mean rating for english books: {meng:.2f}\")\n",
    "print(f\"Mean rating for non-english books: {mneng:.2f}\")\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(eng['average_rating'], neng['average_rating'])\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "if p_value < alpha:\n",
    "    print(\"There is a statistically significant difference in average ratings between English and non-English books.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in average ratings between English and non-English books.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-english books have a bit of a lower rating.\n",
    "\n",
    "Lets go deepr in detail to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
    "deng = lb[['average_rating','language']]\n",
    "\n",
    "# Data preparation\n",
    "deng['average_rating'] = deng['average_rating'].astype(float)\n",
    "deng['language'] = deng['language'].str.lower()  # Ensure uniform case for language comparison\n",
    "\n",
    "# Split data into English and non-English books\n",
    "eng = deng[deng['language'] == 'eng']\n",
    "neng = deng[deng['language'] != 'eng']\n",
    "\n",
    "# Calculate descriptive statistics for each group\n",
    "english_stats = eng['average_rating'].describe()\n",
    "non_english_stats = neng['average_rating'].describe()\n",
    "\n",
    "# Create box plots for each group\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(eng['average_rating'], vert=False)\n",
    "plt.title('English Books')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(neng['average_rating'], vert=False)\n",
    "plt.title('Non-English Books')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Print and interpret the statistics\n",
    "print(\"Statistics for English Books:\")\n",
    "print(english_stats)\n",
    "print(\"\\nStatistics for Non-English Books:\")\n",
    "print(non_english_stats)\n",
    "print(statistucs)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems reasonable to assume that authors with more fans should have more reviews, but maybe their fans are a bit lazy. Confirm or reject this with a convenient statistical test or a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"lighter_authors.json\"\n",
    "ch_size = 100\n",
    "dfs = []\n",
    "\n",
    "# to import only some columns\n",
    "with open(file_path, \"r\") as file:\n",
    "    for ch in pd.read_json(file, lines = True, chunksize = ch_size):\n",
    "        ch = ch[[\"name\", \"works_count\",\"ratings_count\",\"average_rating\",\n",
    "\"text_reviews_count\", \"work_ids\", \"book_ids\",\"id\",\n",
    "\"gender\",\n",
    "\"about\",\"fans_count\"]]\n",
    "        dfs.append(ch)\n",
    "\n",
    "la = pd.concat(dfs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load your dataset with author information, fans, and reviews\n",
    "df = la[['fans_count','ratings_count']]\n",
    "# Calculate Pearson correlation coefficient and p-value\n",
    "correlation, p_value = pearsonr(df['fans_count'], df['ratings_count'])\n",
    "\n",
    "print(f\"Pearson Correlation: {correlation:.2f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < 0.05:  # Using a significance level of 0.05\n",
    "    print(\"There is a statistically significant correlation between the number of fans and the number of reviews.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant correlation between the number of fans and the number of reviews.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Define the dependent and independent variables\n",
    "X = df['fans_count']\n",
    "# X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "y = df['ratings_count']\n",
    "\n",
    "# Build a linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "# plt.plot(X,y,'.')\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.5, label='Data Points')\n",
    "\n",
    "# Add the linear regression line to the plot\n",
    "plt.plot(X, model.predict(X), color='red', label='Linear Regression Line')\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Number of Fans')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.title('Relationship Between Number of Fans and Number of Reviews')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.5, label='Data Points')\n",
    "\n",
    "# Linear regression line\n",
    "coefficients = np.polyfit(X, y, 1)\n",
    "linear_regression = np.poly1d(coefficients)\n",
    "x_range = np.linspace(min(X), max(X), 100)\n",
    "y_predicted = linear_regression(x_range)\n",
    "\n",
    "# Plot the linear regression line\n",
    "plt.plot(x_range, y_predicted, color='red', label='Linear Regression Line')\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Number of Fans')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.title('Relationship Between Number of Fans and Number of Reviews')\n",
    "plt.legend()\n",
    "\n",
    "correlation_coefficient = np.corrcoef(X, y)[0, 1]\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the equation of the linear regression line\n",
    "slope, intercept = coefficients\n",
    "print(f\"Linear Regression Equation: y = {slope:.2f}x + {intercept:.2f}\")\n",
    "print(f\"Correlation Coefficient: {correlation_coefficient:.2f}\")\n",
    "print(linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
